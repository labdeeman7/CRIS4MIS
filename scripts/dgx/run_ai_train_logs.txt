first argument is /nfs/home/talabi/CRIS4MIS/config/endovis2018/cris_r50_bigger_size_data_aug.yaml
2023-04-25 18:30:31 | INFO     | model:41 - Backbone with decay=327, Head=124
2023-04-25 18:30:31 | INFO     | __mp_main__:111 - CRIS(
  (backbone): CLIP(
    (visual): ModifiedResNet(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn3): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
      (relu): ReLU(inplace=True)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (avgpool): Identity()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (attnpool): AttentionPool2d(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (c_proj): Linear(in_features=2048, out_features=1024, bias=True)
        (connect): Sequential(
          (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (text_state_proj): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (neck): FPN(
    (txt_proj): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=False)
      (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f1_v_proj): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (norm_layer): Sequential(
      (0): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
    )
    (f2_v_proj): Sequential(
      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f2_cat): Sequential(
      (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f3_v_proj): Sequential(
      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f3_cat): Sequential(
      (0): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f4_proj5): Sequential(
      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f4_proj4): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (f4_proj3): Sequential(
      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (aggr): Sequential(
      (0): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (coordconv): Sequential(
      (0): CoordConv(
        (conv1): Sequential(
          (0): Conv2d(514, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ffn): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU(inplace=True)
          (2): Dropout(p=0.1, inplace=False)
          (3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          (4): Linear(in_features=2048, out_features=512, bias=True)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ffn): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU(inplace=True)
          (2): Dropout(p=0.1, inplace=False)
          (3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          (4): Linear(in_features=2048, out_features=512, bias=True)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (cross_attn_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ffn): Sequential(
          (0): Linear(in_features=512, out_features=2048, bias=True)
          (1): ReLU(inplace=True)
          (2): Dropout(p=0.1, inplace=False)
          (3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          (4): Linear(in_features=2048, out_features=512, bias=True)
        )
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Projector(
    (vis): Sequential(
      (0): Upsample(scale_factor=2.0, mode=bilinear)
      (1): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (2): Upsample(scale_factor=2.0, mode=bilinear)
      (3): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (txt): Linear(in_features=1024, out_features=2305, bias=True)
  )
)
2023-04-25 18:30:31 | INFO     | __mp_main__:116 - 
+--------------------------------------------------------+--------------------------------+---------+
| Name                                                   | Shape                          | ReqGrad |
+--------------------------------------------------------+--------------------------------+---------+
| backbone.positional_embedding                          | torch.Size([77, 512])          | True    |
| backbone.text_projection                               | torch.Size([512, 1024])        | True    |
| backbone.logit_scale                                   | torch.Size([])                 | True    |
| backbone.visual.conv1.weight                           | torch.Size([32, 3, 3, 3])      | True    |
| backbone.visual.bn1.weight                             | torch.Size([32])               | True    |
| backbone.visual.bn1.bias                               | torch.Size([32])               | True    |
| backbone.visual.conv2.weight                           | torch.Size([32, 32, 3, 3])     | True    |
| backbone.visual.bn2.weight                             | torch.Size([32])               | True    |
| backbone.visual.bn2.bias                               | torch.Size([32])               | True    |
| backbone.visual.conv3.weight                           | torch.Size([64, 32, 3, 3])     | True    |
| backbone.visual.bn3.weight                             | torch.Size([64])               | True    |
| backbone.visual.bn3.bias                               | torch.Size([64])               | True    |
| backbone.visual.layer1.0.conv1.weight                  | torch.Size([64, 64, 1, 1])     | True    |
| backbone.visual.layer1.0.bn1.weight                    | torch.Size([64])               | True    |
| backbone.visual.layer1.0.bn1.bias                      | torch.Size([64])               | True    |
| backbone.visual.layer1.0.conv2.weight                  | torch.Size([64, 64, 3, 3])     | True    |
| backbone.visual.layer1.0.bn2.weight                    | torch.Size([64])               | True    |
| backbone.visual.layer1.0.bn2.bias                      | torch.Size([64])               | True    |
| backbone.visual.layer1.0.conv3.weight                  | torch.Size([256, 64, 1, 1])    | True    |
| backbone.visual.layer1.0.bn3.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer1.0.bn3.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer1.0.downsample.0.weight           | torch.Size([256, 64, 1, 1])    | True    |
| backbone.visual.layer1.0.downsample.1.weight           | torch.Size([256])              | True    |
| backbone.visual.layer1.0.downsample.1.bias             | torch.Size([256])              | True    |
| backbone.visual.layer1.1.conv1.weight                  | torch.Size([64, 256, 1, 1])    | True    |
| backbone.visual.layer1.1.bn1.weight                    | torch.Size([64])               | True    |
| backbone.visual.layer1.1.bn1.bias                      | torch.Size([64])               | True    |
| backbone.visual.layer1.1.conv2.weight                  | torch.Size([64, 64, 3, 3])     | True    |
| backbone.visual.layer1.1.bn2.weight                    | torch.Size([64])               | True    |
| backbone.visual.layer1.1.bn2.bias                      | torch.Size([64])               | True    |
| backbone.visual.layer1.1.conv3.weight                  | torch.Size([256, 64, 1, 1])    | True    |
| backbone.visual.layer1.1.bn3.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer1.1.bn3.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer1.2.conv1.weight                  | torch.Size([64, 256, 1, 1])    | True    |
| backbone.visual.layer1.2.bn1.weight                    | torch.Size([64])               | True    |
| backbone.visual.layer1.2.bn1.bias                      | torch.Size([64])               | True    |
| backbone.visual.layer1.2.conv2.weight                  | torch.Size([64, 64, 3, 3])     | True    |
| backbone.visual.layer1.2.bn2.weight                    | torch.Size([64])               | True    |
| backbone.visual.layer1.2.bn2.bias                      | torch.Size([64])               | True    |
| backbone.visual.layer1.2.conv3.weight                  | torch.Size([256, 64, 1, 1])    | True    |
| backbone.visual.layer1.2.bn3.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer1.2.bn3.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer2.0.conv1.weight                  | torch.Size([128, 256, 1, 1])   | True    |
| backbone.visual.layer2.0.bn1.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.0.bn1.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.0.conv2.weight                  | torch.Size([128, 128, 3, 3])   | True    |
| backbone.visual.layer2.0.bn2.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.0.bn2.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.0.conv3.weight                  | torch.Size([512, 128, 1, 1])   | True    |
| backbone.visual.layer2.0.bn3.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer2.0.bn3.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer2.0.downsample.0.weight           | torch.Size([512, 256, 1, 1])   | True    |
| backbone.visual.layer2.0.downsample.1.weight           | torch.Size([512])              | True    |
| backbone.visual.layer2.0.downsample.1.bias             | torch.Size([512])              | True    |
| backbone.visual.layer2.1.conv1.weight                  | torch.Size([128, 512, 1, 1])   | True    |
| backbone.visual.layer2.1.bn1.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.1.bn1.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.1.conv2.weight                  | torch.Size([128, 128, 3, 3])   | True    |
| backbone.visual.layer2.1.bn2.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.1.bn2.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.1.conv3.weight                  | torch.Size([512, 128, 1, 1])   | True    |
| backbone.visual.layer2.1.bn3.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer2.1.bn3.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer2.2.conv1.weight                  | torch.Size([128, 512, 1, 1])   | True    |
| backbone.visual.layer2.2.bn1.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.2.bn1.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.2.conv2.weight                  | torch.Size([128, 128, 3, 3])   | True    |
| backbone.visual.layer2.2.bn2.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.2.bn2.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.2.conv3.weight                  | torch.Size([512, 128, 1, 1])   | True    |
| backbone.visual.layer2.2.bn3.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer2.2.bn3.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer2.3.conv1.weight                  | torch.Size([128, 512, 1, 1])   | True    |
| backbone.visual.layer2.3.bn1.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.3.bn1.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.3.conv2.weight                  | torch.Size([128, 128, 3, 3])   | True    |
| backbone.visual.layer2.3.bn2.weight                    | torch.Size([128])              | True    |
| backbone.visual.layer2.3.bn2.bias                      | torch.Size([128])              | True    |
| backbone.visual.layer2.3.conv3.weight                  | torch.Size([512, 128, 1, 1])   | True    |
| backbone.visual.layer2.3.bn3.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer2.3.bn3.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer3.0.conv1.weight                  | torch.Size([256, 512, 1, 1])   | True    |
| backbone.visual.layer3.0.bn1.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.0.bn1.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.0.conv2.weight                  | torch.Size([256, 256, 3, 3])   | True    |
| backbone.visual.layer3.0.bn2.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.0.bn2.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.0.conv3.weight                  | torch.Size([1024, 256, 1, 1])  | True    |
| backbone.visual.layer3.0.bn3.weight                    | torch.Size([1024])             | True    |
| backbone.visual.layer3.0.bn3.bias                      | torch.Size([1024])             | True    |
| backbone.visual.layer3.0.downsample.0.weight           | torch.Size([1024, 512, 1, 1])  | True    |
| backbone.visual.layer3.0.downsample.1.weight           | torch.Size([1024])             | True    |
| backbone.visual.layer3.0.downsample.1.bias             | torch.Size([1024])             | True    |
| backbone.visual.layer3.1.conv1.weight                  | torch.Size([256, 1024, 1, 1])  | True    |
| backbone.visual.layer3.1.bn1.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.1.bn1.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.1.conv2.weight                  | torch.Size([256, 256, 3, 3])   | True    |
| backbone.visual.layer3.1.bn2.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.1.bn2.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.1.conv3.weight                  | torch.Size([1024, 256, 1, 1])  | True    |
| backbone.visual.layer3.1.bn3.weight                    | torch.Size([1024])             | True    |
| backbone.visual.layer3.1.bn3.bias                      | torch.Size([1024])             | True    |
| backbone.visual.layer3.2.conv1.weight                  | torch.Size([256, 1024, 1, 1])  | True    |
| backbone.visual.layer3.2.bn1.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.2.bn1.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.2.conv2.weight                  | torch.Size([256, 256, 3, 3])   | True    |
| backbone.visual.layer3.2.bn2.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.2.bn2.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.2.conv3.weight                  | torch.Size([1024, 256, 1, 1])  | True    |
| backbone.visual.layer3.2.bn3.weight                    | torch.Size([1024])             | True    |
| backbone.visual.layer3.2.bn3.bias                      | torch.Size([1024])             | True    |
| backbone.visual.layer3.3.conv1.weight                  | torch.Size([256, 1024, 1, 1])  | True    |
| backbone.visual.layer3.3.bn1.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.3.bn1.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.3.conv2.weight                  | torch.Size([256, 256, 3, 3])   | True    |
| backbone.visual.layer3.3.bn2.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.3.bn2.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.3.conv3.weight                  | torch.Size([1024, 256, 1, 1])  | True    |
| backbone.visual.layer3.3.bn3.weight                    | torch.Size([1024])             | True    |
| backbone.visual.layer3.3.bn3.bias                      | torch.Size([1024])             | True    |
| backbone.visual.layer3.4.conv1.weight                  | torch.Size([256, 1024, 1, 1])  | True    |
| backbone.visual.layer3.4.bn1.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.4.bn1.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.4.conv2.weight                  | torch.Size([256, 256, 3, 3])   | True    |
| backbone.visual.layer3.4.bn2.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.4.bn2.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.4.conv3.weight                  | torch.Size([1024, 256, 1, 1])  | True    |
| backbone.visual.layer3.4.bn3.weight                    | torch.Size([1024])             | True    |
| backbone.visual.layer3.4.bn3.bias                      | torch.Size([1024])             | True    |
| backbone.visual.layer3.5.conv1.weight                  | torch.Size([256, 1024, 1, 1])  | True    |
| backbone.visual.layer3.5.bn1.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.5.bn1.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.5.conv2.weight                  | torch.Size([256, 256, 3, 3])   | True    |
| backbone.visual.layer3.5.bn2.weight                    | torch.Size([256])              | True    |
| backbone.visual.layer3.5.bn2.bias                      | torch.Size([256])              | True    |
| backbone.visual.layer3.5.conv3.weight                  | torch.Size([1024, 256, 1, 1])  | True    |
| backbone.visual.layer3.5.bn3.weight                    | torch.Size([1024])             | True    |
| backbone.visual.layer3.5.bn3.bias                      | torch.Size([1024])             | True    |
| backbone.visual.layer4.0.conv1.weight                  | torch.Size([512, 1024, 1, 1])  | True    |
| backbone.visual.layer4.0.bn1.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer4.0.bn1.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer4.0.conv2.weight                  | torch.Size([512, 512, 3, 3])   | True    |
| backbone.visual.layer4.0.bn2.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer4.0.bn2.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer4.0.conv3.weight                  | torch.Size([2048, 512, 1, 1])  | True    |
| backbone.visual.layer4.0.bn3.weight                    | torch.Size([2048])             | True    |
| backbone.visual.layer4.0.bn3.bias                      | torch.Size([2048])             | True    |
| backbone.visual.layer4.0.downsample.0.weight           | torch.Size([2048, 1024, 1, 1]) | True    |
| backbone.visual.layer4.0.downsample.1.weight           | torch.Size([2048])             | True    |
| backbone.visual.layer4.0.downsample.1.bias             | torch.Size([2048])             | True    |
| backbone.visual.layer4.1.conv1.weight                  | torch.Size([512, 2048, 1, 1])  | True    |
| backbone.visual.layer4.1.bn1.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer4.1.bn1.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer4.1.conv2.weight                  | torch.Size([512, 512, 3, 3])   | True    |
| backbone.visual.layer4.1.bn2.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer4.1.bn2.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer4.1.conv3.weight                  | torch.Size([2048, 512, 1, 1])  | True    |
| backbone.visual.layer4.1.bn3.weight                    | torch.Size([2048])             | True    |
| backbone.visual.layer4.1.bn3.bias                      | torch.Size([2048])             | True    |
| backbone.visual.layer4.2.conv1.weight                  | torch.Size([512, 2048, 1, 1])  | True    |
| backbone.visual.layer4.2.bn1.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer4.2.bn1.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer4.2.conv2.weight                  | torch.Size([512, 512, 3, 3])   | True    |
| backbone.visual.layer4.2.bn2.weight                    | torch.Size([512])              | True    |
| backbone.visual.layer4.2.bn2.bias                      | torch.Size([512])              | True    |
| backbone.visual.layer4.2.conv3.weight                  | torch.Size([2048, 512, 1, 1])  | True    |
| backbone.visual.layer4.2.bn3.weight                    | torch.Size([2048])             | True    |
| backbone.visual.layer4.2.bn3.bias                      | torch.Size([2048])             | True    |
| backbone.visual.attnpool.positional_embedding          | torch.Size([50, 2048])         | True    |
| backbone.visual.attnpool.k_proj.weight                 | torch.Size([2048, 2048])       | True    |
| backbone.visual.attnpool.k_proj.bias                   | torch.Size([2048])             | True    |
| backbone.visual.attnpool.q_proj.weight                 | torch.Size([2048, 2048])       | True    |
| backbone.visual.attnpool.q_proj.bias                   | torch.Size([2048])             | True    |
| backbone.visual.attnpool.v_proj.weight                 | torch.Size([2048, 2048])       | True    |
| backbone.visual.attnpool.v_proj.bias                   | torch.Size([2048])             | True    |
| backbone.visual.attnpool.c_proj.weight                 | torch.Size([1024, 2048])       | True    |
| backbone.visual.attnpool.c_proj.bias                   | torch.Size([1024])             | True    |
| backbone.visual.attnpool.connect.0.weight              | torch.Size([1024, 2048, 1, 1]) | True    |
| backbone.visual.attnpool.connect.1.weight              | torch.Size([1024])             | True    |
| backbone.visual.attnpool.connect.1.bias                | torch.Size([1024])             | True    |
| backbone.transformer.resblocks.0.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.0.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.0.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.0.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.0.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.0.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.0.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.0.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.0.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.0.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.0.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.0.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.1.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.1.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.1.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.1.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.1.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.1.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.1.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.1.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.1.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.1.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.1.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.1.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.2.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.2.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.2.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.2.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.2.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.2.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.2.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.2.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.2.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.2.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.2.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.2.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.3.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.3.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.3.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.3.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.3.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.3.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.3.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.3.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.3.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.3.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.3.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.3.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.4.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.4.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.4.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.4.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.4.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.4.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.4.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.4.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.4.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.4.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.4.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.4.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.5.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.5.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.5.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.5.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.5.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.5.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.5.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.5.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.5.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.5.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.5.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.5.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.6.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.6.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.6.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.6.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.6.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.6.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.6.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.6.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.6.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.6.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.6.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.6.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.7.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.7.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.7.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.7.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.7.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.7.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.7.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.7.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.7.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.7.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.7.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.7.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.8.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.8.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.8.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.8.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.8.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.8.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.8.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.8.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.8.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.8.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.8.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.8.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.9.attn.in_proj_weight   | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.9.attn.in_proj_bias     | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.9.attn.out_proj.weight  | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.9.attn.out_proj.bias    | torch.Size([512])              | True    |
| backbone.transformer.resblocks.9.ln_1.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.9.ln_1.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.9.mlp.c_fc.weight       | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.9.mlp.c_fc.bias         | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.9.mlp.c_proj.weight     | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.9.mlp.c_proj.bias       | torch.Size([512])              | True    |
| backbone.transformer.resblocks.9.ln_2.weight           | torch.Size([512])              | True    |
| backbone.transformer.resblocks.9.ln_2.bias             | torch.Size([512])              | True    |
| backbone.transformer.resblocks.10.attn.in_proj_weight  | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.10.attn.in_proj_bias    | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.10.attn.out_proj.weight | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.10.attn.out_proj.bias   | torch.Size([512])              | True    |
| backbone.transformer.resblocks.10.ln_1.weight          | torch.Size([512])              | True    |
| backbone.transformer.resblocks.10.ln_1.bias            | torch.Size([512])              | True    |
| backbone.transformer.resblocks.10.mlp.c_fc.weight      | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.10.mlp.c_fc.bias        | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.10.mlp.c_proj.weight    | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.10.mlp.c_proj.bias      | torch.Size([512])              | True    |
| backbone.transformer.resblocks.10.ln_2.weight          | torch.Size([512])              | True    |
| backbone.transformer.resblocks.10.ln_2.bias            | torch.Size([512])              | True    |
| backbone.transformer.resblocks.11.attn.in_proj_weight  | torch.Size([1536, 512])        | True    |
| backbone.transformer.resblocks.11.attn.in_proj_bias    | torch.Size([1536])             | True    |
| backbone.transformer.resblocks.11.attn.out_proj.weight | torch.Size([512, 512])         | True    |
| backbone.transformer.resblocks.11.attn.out_proj.bias   | torch.Size([512])              | True    |
| backbone.transformer.resblocks.11.ln_1.weight          | torch.Size([512])              | True    |
| backbone.transformer.resblocks.11.ln_1.bias            | torch.Size([512])              | True    |
| backbone.transformer.resblocks.11.mlp.c_fc.weight      | torch.Size([2048, 512])        | True    |
| backbone.transformer.resblocks.11.mlp.c_fc.bias        | torch.Size([2048])             | True    |
| backbone.transformer.resblocks.11.mlp.c_proj.weight    | torch.Size([512, 2048])        | True    |
| backbone.transformer.resblocks.11.mlp.c_proj.bias      | torch.Size([512])              | True    |
| backbone.transformer.resblocks.11.ln_2.weight          | torch.Size([512])              | True    |
| backbone.transformer.resblocks.11.ln_2.bias            | torch.Size([512])              | True    |
| backbone.token_embedding.weight                        | torch.Size([49408, 512])       | True    |
| backbone.ln_final.weight                               | torch.Size([512])              | True    |
| backbone.ln_final.bias                                 | torch.Size([512])              | True    |
| backbone.text_state_proj.weight                        | torch.Size([1024, 1024])       | True    |
| backbone.text_state_proj.bias                          | torch.Size([1024])             | True    |
| neck.txt_proj.0.weight                                 | torch.Size([1024, 1024])       | True    |
| neck.txt_proj.1.weight                                 | torch.Size([1024])             | True    |
| neck.txt_proj.1.bias                                   | torch.Size([1024])             | True    |
| neck.f1_v_proj.0.weight                                | torch.Size([1024, 1024, 1, 1]) | True    |
| neck.f1_v_proj.1.weight                                | torch.Size([1024])             | True    |
| neck.f1_v_proj.1.bias                                  | torch.Size([1024])             | True    |
| neck.norm_layer.0.weight                               | torch.Size([1024])             | True    |
| neck.norm_layer.0.bias                                 | torch.Size([1024])             | True    |
| neck.f2_v_proj.0.weight                                | torch.Size([512, 1024, 3, 3])  | True    |
| neck.f2_v_proj.1.weight                                | torch.Size([512])              | True    |
| neck.f2_v_proj.1.bias                                  | torch.Size([512])              | True    |
| neck.f2_cat.0.weight                                   | torch.Size([512, 1536, 1, 1])  | True    |
| neck.f2_cat.1.weight                                   | torch.Size([512])              | True    |
| neck.f2_cat.1.bias                                     | torch.Size([512])              | True    |
| neck.f3_v_proj.0.weight                                | torch.Size([256, 512, 3, 3])   | True    |
| neck.f3_v_proj.1.weight                                | torch.Size([256])              | True    |
| neck.f3_v_proj.1.bias                                  | torch.Size([256])              | True    |
| neck.f3_cat.0.weight                                   | torch.Size([512, 768, 1, 1])   | True    |
| neck.f3_cat.1.weight                                   | torch.Size([512])              | True    |
| neck.f3_cat.1.bias                                     | torch.Size([512])              | True    |
| neck.f4_proj5.0.weight                                 | torch.Size([512, 1024, 3, 3])  | True    |
| neck.f4_proj5.1.weight                                 | torch.Size([512])              | True    |
| neck.f4_proj5.1.bias                                   | torch.Size([512])              | True    |
| neck.f4_proj4.0.weight                                 | torch.Size([512, 512, 3, 3])   | True    |
| neck.f4_proj4.1.weight                                 | torch.Size([512])              | True    |
| neck.f4_proj4.1.bias                                   | torch.Size([512])              | True    |
| neck.f4_proj3.0.weight                                 | torch.Size([512, 512, 3, 3])   | True    |
| neck.f4_proj3.1.weight                                 | torch.Size([512])              | True    |
| neck.f4_proj3.1.bias                                   | torch.Size([512])              | True    |
| neck.aggr.0.weight                                     | torch.Size([512, 1536, 1, 1])  | True    |
| neck.aggr.1.weight                                     | torch.Size([512])              | True    |
| neck.aggr.1.bias                                       | torch.Size([512])              | True    |
| neck.coordconv.0.conv1.0.weight                        | torch.Size([512, 514, 3, 3])   | True    |
| neck.coordconv.0.conv1.1.weight                        | torch.Size([512])              | True    |
| neck.coordconv.0.conv1.1.bias                          | torch.Size([512])              | True    |
| neck.coordconv.1.0.weight                              | torch.Size([512, 512, 3, 3])   | True    |
| neck.coordconv.1.1.weight                              | torch.Size([512])              | True    |
| neck.coordconv.1.1.bias                                | torch.Size([512])              | True    |
| decoder.layers.0.self_attn_norm.weight                 | torch.Size([512])              | True    |
| decoder.layers.0.self_attn_norm.bias                   | torch.Size([512])              | True    |
| decoder.layers.0.cross_attn_norm.weight                | torch.Size([512])              | True    |
| decoder.layers.0.cross_attn_norm.bias                  | torch.Size([512])              | True    |
| decoder.layers.0.self_attn.in_proj_weight              | torch.Size([1536, 512])        | True    |
| decoder.layers.0.self_attn.in_proj_bias                | torch.Size([1536])             | True    |
| decoder.layers.0.self_attn.out_proj.weight             | torch.Size([512, 512])         | True    |
| decoder.layers.0.self_attn.out_proj.bias               | torch.Size([512])              | True    |
| decoder.layers.0.multihead_attn.in_proj_weight         | torch.Size([1536, 512])        | True    |
| decoder.layers.0.multihead_attn.in_proj_bias           | torch.Size([1536])             | True    |
| decoder.layers.0.multihead_attn.out_proj.weight        | torch.Size([512, 512])         | True    |
| decoder.layers.0.multihead_attn.out_proj.bias          | torch.Size([512])              | True    |
| decoder.layers.0.ffn.0.weight                          | torch.Size([2048, 512])        | True    |
| decoder.layers.0.ffn.0.bias                            | torch.Size([2048])             | True    |
| decoder.layers.0.ffn.3.weight                          | torch.Size([2048])             | True    |
| decoder.layers.0.ffn.3.bias                            | torch.Size([2048])             | True    |
| decoder.layers.0.ffn.4.weight                          | torch.Size([512, 2048])        | True    |
| decoder.layers.0.ffn.4.bias                            | torch.Size([512])              | True    |
| decoder.layers.0.norm1.weight                          | torch.Size([512])              | True    |
| decoder.layers.0.norm1.bias                            | torch.Size([512])              | True    |
| decoder.layers.0.norm2.weight                          | torch.Size([512])              | True    |
| decoder.layers.0.norm2.bias                            | torch.Size([512])              | True    |
| decoder.layers.0.norm3.weight                          | torch.Size([512])              | True    |
| decoder.layers.0.norm3.bias                            | torch.Size([512])              | True    |
| decoder.layers.1.self_attn_norm.weight                 | torch.Size([512])              | True    |
| decoder.layers.1.self_attn_norm.bias                   | torch.Size([512])              | True    |
| decoder.layers.1.cross_attn_norm.weight                | torch.Size([512])              | True    |
| decoder.layers.1.cross_attn_norm.bias                  | torch.Size([512])              | True    |
| decoder.layers.1.self_attn.in_proj_weight              | torch.Size([1536, 512])        | True    |
| decoder.layers.1.self_attn.in_proj_bias                | torch.Size([1536])             | True    |
| decoder.layers.1.self_attn.out_proj.weight             | torch.Size([512, 512])         | True    |
| decoder.layers.1.self_attn.out_proj.bias               | torch.Size([512])              | True    |
| decoder.layers.1.multihead_attn.in_proj_weight         | torch.Size([1536, 512])        | True    |
| decoder.layers.1.multihead_attn.in_proj_bias           | torch.Size([1536])             | True    |
| decoder.layers.1.multihead_attn.out_proj.weight        | torch.Size([512, 512])         | True    |
| decoder.layers.1.multihead_attn.out_proj.bias          | torch.Size([512])              | True    |
| decoder.layers.1.ffn.0.weight                          | torch.Size([2048, 512])        | True    |
| decoder.layers.1.ffn.0.bias                            | torch.Size([2048])             | True    |
| decoder.layers.1.ffn.3.weight                          | torch.Size([2048])             | True    |
| decoder.layers.1.ffn.3.bias                            | torch.Size([2048])             | True    |
| decoder.layers.1.ffn.4.weight                          | torch.Size([512, 2048])        | True    |
| decoder.layers.1.ffn.4.bias                            | torch.Size([512])              | True    |
| decoder.layers.1.norm1.weight                          | torch.Size([512])              | True    |
| decoder.layers.1.norm1.bias                            | torch.Size([512])              | True    |
| decoder.layers.1.norm2.weight                          | torch.Size([512])              | True    |
| decoder.layers.1.norm2.bias                            | torch.Size([512])              | True    |
| decoder.layers.1.norm3.weight                          | torch.Size([512])              | True    |
| decoder.layers.1.norm3.bias                            | torch.Size([512])              | True    |
| decoder.layers.2.self_attn_norm.weight                 | torch.Size([512])              | True    |
| decoder.layers.2.self_attn_norm.bias                   | torch.Size([512])              | True    |
| decoder.layers.2.cross_attn_norm.weight                | torch.Size([512])              | True    |
| decoder.layers.2.cross_attn_norm.bias                  | torch.Size([512])              | True    |
| decoder.layers.2.self_attn.in_proj_weight              | torch.Size([1536, 512])        | True    |
| decoder.layers.2.self_attn.in_proj_bias                | torch.Size([1536])             | True    |
| decoder.layers.2.self_attn.out_proj.weight             | torch.Size([512, 512])         | True    |
| decoder.layers.2.self_attn.out_proj.bias               | torch.Size([512])              | True    |
| decoder.layers.2.multihead_attn.in_proj_weight         | torch.Size([1536, 512])        | True    |
| decoder.layers.2.multihead_attn.in_proj_bias           | torch.Size([1536])             | True    |
| decoder.layers.2.multihead_attn.out_proj.weight        | torch.Size([512, 512])         | True    |
| decoder.layers.2.multihead_attn.out_proj.bias          | torch.Size([512])              | True    |
| decoder.layers.2.ffn.0.weight                          | torch.Size([2048, 512])        | True    |
| decoder.layers.2.ffn.0.bias                            | torch.Size([2048])             | True    |
| decoder.layers.2.ffn.3.weight                          | torch.Size([2048])             | True    |
| decoder.layers.2.ffn.3.bias                            | torch.Size([2048])             | True    |
| decoder.layers.2.ffn.4.weight                          | torch.Size([512, 2048])        | True    |
| decoder.layers.2.ffn.4.bias                            | torch.Size([512])              | True    |
| decoder.layers.2.norm1.weight                          | torch.Size([512])              | True    |
| decoder.layers.2.norm1.bias                            | torch.Size([512])              | True    |
| decoder.layers.2.norm2.weight                          | torch.Size([512])              | True    |
| decoder.layers.2.norm2.bias                            | torch.Size([512])              | True    |
| decoder.layers.2.norm3.weight                          | torch.Size([512])              | True    |
| decoder.layers.2.norm3.bias                            | torch.Size([512])              | True    |
| decoder.norm.weight                                    | torch.Size([512])              | True    |
| decoder.norm.bias                                      | torch.Size([512])              | True    |
| proj.vis.1.0.weight                                    | torch.Size([512, 512, 3, 3])   | True    |
| proj.vis.1.1.weight                                    | torch.Size([512])              | True    |
| proj.vis.1.1.bias                                      | torch.Size([512])              | True    |
| proj.vis.3.0.weight                                    | torch.Size([256, 512, 3, 3])   | True    |
| proj.vis.3.1.weight                                    | torch.Size([256])              | True    |
| proj.vis.3.1.bias                                      | torch.Size([256])              | True    |
| proj.vis.4.weight                                      | torch.Size([256, 256, 1, 1])   | True    |
| proj.vis.4.bias                                        | torch.Size([256])              | True    |
| proj.txt.weight                                        | torch.Size([2305, 1024])       | True    |
| proj.txt.bias                                          | torch.Size([2305])             | True    |
+--------------------------------------------------------+--------------------------------+---------+
2023-04-25 18:35:10 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 100/1254]  Batch=2.13 (2.73)  Data=0.00 (0.18)  Lr=0.000100  Loss=0.2849 (0.6689)  IoU=6.52 (10.89)  Prec@50=6.25 (10.56)
2023-04-25 18:38:54 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 200/1254]  Batch=2.04 (2.48)  Data=0.00 (0.09)  Lr=0.000100  Loss=0.2214 (0.4868)  IoU=17.59 (18.02)  Prec@50=12.50 (17.97)
2023-04-25 18:42:49 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 300/1254]  Batch=2.96 (2.44)  Data=0.00 (0.06)  Lr=0.000100  Loss=0.3472 (0.4122)  IoU=36.95 (22.58)  Prec@50=43.75 (23.17)
2023-04-25 18:46:44 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 400/1254]  Batch=3.31 (2.42)  Data=0.00 (0.05)  Lr=0.000100  Loss=0.2387 (0.3701)  IoU=43.92 (26.17)  Prec@50=50.00 (27.42)
2023-04-25 18:50:34 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 500/1254]  Batch=2.09 (2.39)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.2230 (0.3423)  IoU=34.19 (28.68)  Prec@50=31.25 (30.34)
2023-04-25 18:54:17 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 600/1254]  Batch=1.75 (2.37)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.2507 (0.3209)  IoU=38.27 (31.06)  Prec@50=43.75 (32.92)
2023-04-25 18:58:02 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 700/1254]  Batch=2.44 (2.35)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.2245 (0.3070)  IoU=50.59 (32.88)  Prec@50=56.25 (34.79)
2023-04-25 19:01:34 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 800/1254]  Batch=2.87 (2.32)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1824 (0.2949)  IoU=62.28 (34.35)  Prec@50=75.00 (36.40)
2023-04-25 19:05:06 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [ 900/1254]  Batch=2.10 (2.30)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1823 (0.2852)  IoU=45.68 (35.70)  Prec@50=50.00 (37.93)
2023-04-25 19:08:42 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [1000/1254]  Batch=1.93 (2.28)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1726 (0.2767)  IoU=49.48 (36.90)  Prec@50=56.25 (39.29)
2023-04-25 19:12:21 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [1100/1254]  Batch=2.20 (2.28)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1494 (0.2683)  IoU=40.70 (37.98)  Prec@50=37.50 (40.62)
2023-04-25 19:15:50 | INFO     | utils.misc:106 - Training: Epoch=[1/50] [1200/1254]  Batch=1.87 (2.26)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1975 (0.2613)  IoU=54.56 (39.14)  Prec@50=56.25 (41.95)
2023-04-25 19:31:38 | INFO     | engine.engine:148 - Evaluation: Epoch=[1/50]  IoU=48.20  Pr@50: 50.71  Pr@60: 42.22  Pr@70: 33.93  Pr@80: 24.41  Pr@90: 12.06  
2023-04-25 19:35:47 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 100/1254]  Batch=2.22 (2.31)  Data=0.00 (0.16)  Lr=0.000100  Loss=0.1385 (0.1552)  IoU=54.95 (54.18)  Prec@50=56.25 (60.50)
2023-04-25 19:39:01 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 200/1254]  Batch=2.03 (2.13)  Data=0.00 (0.08)  Lr=0.000100  Loss=0.1231 (0.1469)  IoU=67.36 (56.01)  Prec@50=75.00 (62.56)
2023-04-25 19:42:22 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 300/1254]  Batch=2.14 (2.09)  Data=0.00 (0.05)  Lr=0.000100  Loss=0.1668 (0.1438)  IoU=60.54 (57.19)  Prec@50=62.50 (63.69)
2023-04-25 19:45:41 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 400/1254]  Batch=1.83 (2.06)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.1791 (0.1404)  IoU=72.09 (58.05)  Prec@50=87.50 (64.75)
2023-04-25 19:49:03 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 500/1254]  Batch=1.36 (2.05)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.1245 (0.1368)  IoU=57.16 (58.88)  Prec@50=56.25 (65.71)
2023-04-25 19:52:32 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 600/1254]  Batch=1.39 (2.06)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.1373 (0.1340)  IoU=56.73 (59.34)  Prec@50=50.00 (66.02)
2023-04-25 19:56:00 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 700/1254]  Batch=1.14 (2.06)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1162 (0.1310)  IoU=56.74 (59.89)  Prec@50=62.50 (66.59)
2023-04-25 19:59:14 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 800/1254]  Batch=1.52 (2.05)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1375 (0.1299)  IoU=70.14 (60.19)  Prec@50=87.50 (66.97)
2023-04-25 20:02:39 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [ 900/1254]  Batch=1.46 (2.05)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1180 (0.1282)  IoU=69.29 (60.55)  Prec@50=87.50 (67.50)
2023-04-25 20:06:10 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [1000/1254]  Batch=3.63 (2.05)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0877 (0.1265)  IoU=68.46 (60.93)  Prec@50=75.00 (67.94)
2023-04-25 20:09:40 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [1100/1254]  Batch=1.87 (2.06)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1347 (0.1243)  IoU=69.32 (61.34)  Prec@50=68.75 (68.39)
2023-04-25 20:13:34 | INFO     | utils.misc:106 - Training: Epoch=[2/50] [1200/1254]  Batch=1.28 (2.08)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.1015 (0.1227)  IoU=60.47 (61.72)  Prec@50=62.50 (68.83)
2023-04-25 20:29:35 | INFO     | engine.engine:148 - Evaluation: Epoch=[2/50]  IoU=58.64  Pr@50: 63.80  Pr@60: 54.27  Pr@70: 44.57  Pr@80: 36.19  Pr@90: 21.29  
2023-04-25 20:33:50 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 100/1254]  Batch=2.49 (2.39)  Data=0.00 (0.18)  Lr=0.000100  Loss=0.0942 (0.0944)  IoU=66.92 (66.24)  Prec@50=75.00 (73.75)
2023-04-25 20:37:23 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 200/1254]  Batch=2.76 (2.26)  Data=0.00 (0.09)  Lr=0.000100  Loss=0.0626 (0.0919)  IoU=64.50 (66.78)  Prec@50=75.00 (74.78)
2023-04-25 20:40:49 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 300/1254]  Batch=2.33 (2.19)  Data=0.00 (0.06)  Lr=0.000100  Loss=0.0744 (0.0936)  IoU=64.48 (67.37)  Prec@50=75.00 (75.79)
2023-04-25 20:44:23 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 400/1254]  Batch=2.65 (2.18)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0836 (0.0924)  IoU=56.63 (67.96)  Prec@50=62.50 (76.45)
2023-04-25 20:48:00 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 500/1254]  Batch=1.57 (2.18)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0795 (0.0915)  IoU=72.11 (68.50)  Prec@50=87.50 (77.22)
2023-04-25 20:51:40 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 600/1254]  Batch=1.79 (2.18)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.1339 (0.0910)  IoU=68.55 (68.75)  Prec@50=75.00 (77.48)
2023-04-25 20:55:12 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 700/1254]  Batch=1.70 (2.17)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0816 (0.0906)  IoU=63.47 (68.90)  Prec@50=75.00 (77.69)
2023-04-25 20:58:38 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 800/1254]  Batch=1.60 (2.16)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0682 (0.0893)  IoU=81.27 (69.17)  Prec@50=93.75 (77.91)
2023-04-25 21:02:06 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [ 900/1254]  Batch=1.98 (2.15)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0685 (0.0883)  IoU=77.30 (69.39)  Prec@50=93.75 (78.29)
2023-04-25 21:05:25 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [1000/1254]  Batch=2.18 (2.13)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0733 (0.0874)  IoU=78.43 (69.65)  Prec@50=93.75 (78.61)
2023-04-25 21:08:56 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [1100/1254]  Batch=2.10 (2.13)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0856 (0.0869)  IoU=66.98 (69.70)  Prec@50=81.25 (78.59)
2023-04-25 21:12:33 | INFO     | utils.misc:106 - Training: Epoch=[3/50] [1200/1254]  Batch=1.37 (2.13)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0677 (0.0864)  IoU=57.57 (69.85)  Prec@50=56.25 (78.74)
2023-04-25 21:19:24 | INFO     | engine.engine:148 - Evaluation: Epoch=[3/50]  IoU=61.98  Pr@50: 69.07  Pr@60: 59.01  Pr@70: 49.02  Pr@80: 39.74  Pr@90: 23.91  
2023-04-25 21:20:52 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 100/1254]  Batch=0.73 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0535 (0.0751)  IoU=65.13 (72.39)  Prec@50=68.75 (82.19)
2023-04-25 21:22:04 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 200/1254]  Batch=0.74 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0841 (0.0725)  IoU=88.71 (72.80)  Prec@50=100.00 (82.28)
2023-04-25 21:23:15 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 300/1254]  Batch=0.69 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0857 (0.0720)  IoU=69.30 (73.26)  Prec@50=68.75 (83.00)
2023-04-25 21:24:26 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 400/1254]  Batch=0.69 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0580 (0.0715)  IoU=70.49 (73.03)  Prec@50=81.25 (82.69)
2023-04-25 21:25:38 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 500/1254]  Batch=0.76 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0550 (0.0711)  IoU=76.45 (73.43)  Prec@50=93.75 (83.06)
2023-04-25 21:26:52 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 600/1254]  Batch=0.76 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0776 (0.0698)  IoU=84.80 (73.73)  Prec@50=100.00 (83.31)
2023-04-25 21:28:04 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 700/1254]  Batch=0.69 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0544 (0.0694)  IoU=77.45 (73.93)  Prec@50=87.50 (83.52)
2023-04-25 21:29:14 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 800/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0552 (0.0686)  IoU=77.77 (74.24)  Prec@50=81.25 (83.94)
2023-04-25 21:30:24 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [ 900/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0470 (0.0687)  IoU=78.17 (74.35)  Prec@50=81.25 (84.06)
2023-04-25 21:31:33 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [1000/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0655 (0.0686)  IoU=85.17 (74.42)  Prec@50=100.00 (84.17)
2023-04-25 21:32:43 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [1100/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0506 (0.0687)  IoU=66.67 (74.38)  Prec@50=68.75 (84.18)
2023-04-25 21:33:52 | INFO     | utils.misc:106 - Training: Epoch=[4/50] [1200/1254]  Batch=0.77 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0575 (0.0686)  IoU=70.31 (74.44)  Prec@50=75.00 (84.29)
2023-04-25 21:39:09 | INFO     | engine.engine:148 - Evaluation: Epoch=[4/50]  IoU=63.22  Pr@50: 70.85  Pr@60: 61.36  Pr@70: 51.70  Pr@80: 42.42  Pr@90: 26.11  
2023-04-25 21:40:38 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 100/1254]  Batch=0.63 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0534 (0.0601)  IoU=81.53 (76.66)  Prec@50=93.75 (87.00)
2023-04-25 21:41:48 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 200/1254]  Batch=0.69 (0.73)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0503 (0.0599)  IoU=76.00 (76.35)  Prec@50=93.75 (86.69)
2023-04-25 21:42:57 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 300/1254]  Batch=0.72 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0595 (0.0584)  IoU=69.08 (76.38)  Prec@50=75.00 (86.46)
2023-04-25 21:44:07 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 400/1254]  Batch=0.63 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0762 (0.0580)  IoU=69.15 (76.33)  Prec@50=75.00 (86.27)
2023-04-25 21:45:16 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 500/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.1298 (0.0579)  IoU=69.52 (76.82)  Prec@50=75.00 (86.84)
2023-04-25 21:46:27 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 600/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0574 (0.0578)  IoU=80.49 (76.85)  Prec@50=87.50 (86.80)
2023-04-25 21:47:36 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 700/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0779 (0.0583)  IoU=79.31 (77.06)  Prec@50=81.25 (87.04)
2023-04-25 21:48:46 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 800/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0534 (0.0581)  IoU=73.49 (76.97)  Prec@50=81.25 (87.05)
2023-04-25 21:49:56 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [ 900/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0674 (0.0579)  IoU=84.25 (76.93)  Prec@50=100.00 (86.97)
2023-04-25 21:51:06 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [1000/1254]  Batch=0.66 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0597 (0.0580)  IoU=85.42 (76.96)  Prec@50=100.00 (87.00)
2023-04-25 21:52:16 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [1100/1254]  Batch=0.66 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0508 (0.0578)  IoU=70.51 (77.02)  Prec@50=75.00 (86.99)
2023-04-25 21:53:26 | INFO     | utils.misc:106 - Training: Epoch=[5/50] [1200/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0396 (0.0579)  IoU=82.22 (77.00)  Prec@50=93.75 (86.98)
2023-04-25 21:58:42 | INFO     | engine.engine:148 - Evaluation: Epoch=[5/50]  IoU=65.74  Pr@50: 73.88  Pr@60: 65.57  Pr@70: 56.67  Pr@80: 45.56  Pr@90: 28.17  
2023-04-25 22:00:11 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 100/1254]  Batch=0.73 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0440 (0.0615)  IoU=81.64 (77.69)  Prec@50=87.50 (87.50)
2023-04-25 22:01:21 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 200/1254]  Batch=0.73 (0.74)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0618 (0.0555)  IoU=82.16 (78.66)  Prec@50=100.00 (88.75)
2023-04-25 22:02:31 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 300/1254]  Batch=0.70 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0366 (0.0555)  IoU=78.46 (78.31)  Prec@50=87.50 (88.54)
2023-04-25 22:03:40 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 400/1254]  Batch=0.69 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0616 (0.0541)  IoU=83.05 (78.84)  Prec@50=87.50 (88.94)
2023-04-25 22:04:50 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 500/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0291 (0.0539)  IoU=87.38 (78.77)  Prec@50=100.00 (88.60)
2023-04-25 22:05:59 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 600/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0695 (0.0530)  IoU=84.23 (78.78)  Prec@50=100.00 (88.64)
2023-04-25 22:07:09 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 700/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0345 (0.0528)  IoU=76.28 (78.84)  Prec@50=87.50 (88.74)
2023-04-25 22:08:19 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 800/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0567 (0.0528)  IoU=77.93 (78.67)  Prec@50=87.50 (88.62)
2023-04-25 22:09:28 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [ 900/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0774 (0.0525)  IoU=69.42 (78.64)  Prec@50=81.25 (88.51)
2023-04-25 22:10:39 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [1000/1254]  Batch=0.73 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0950 (0.0525)  IoU=72.04 (78.66)  Prec@50=68.75 (88.52)
2023-04-25 22:11:48 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [1100/1254]  Batch=0.67 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0447 (0.0527)  IoU=74.17 (78.72)  Prec@50=81.25 (88.59)
2023-04-25 22:12:58 | INFO     | utils.misc:106 - Training: Epoch=[6/50] [1200/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0379 (0.0526)  IoU=83.66 (78.74)  Prec@50=100.00 (88.57)
2023-04-25 22:18:15 | INFO     | engine.engine:148 - Evaluation: Epoch=[6/50]  IoU=63.60  Pr@50: 71.15  Pr@60: 64.00  Pr@70: 55.43  Pr@80: 45.60  Pr@90: 29.38  
2023-04-25 22:19:40 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 100/1254]  Batch=0.71 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0380 (0.0428)  IoU=66.60 (79.35)  Prec@50=75.00 (88.31)
2023-04-25 22:20:50 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 200/1254]  Batch=0.69 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0455 (0.0441)  IoU=81.56 (79.81)  Prec@50=87.50 (88.69)
2023-04-25 22:21:59 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 300/1254]  Batch=0.68 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0490 (0.0439)  IoU=77.45 (80.13)  Prec@50=81.25 (89.38)
2023-04-25 22:23:09 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 400/1254]  Batch=0.72 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0408 (0.0437)  IoU=63.44 (80.10)  Prec@50=75.00 (89.62)
2023-04-25 22:24:18 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 500/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0753 (0.0436)  IoU=82.17 (80.11)  Prec@50=93.75 (89.67)
2023-04-25 22:25:28 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 600/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0483 (0.0444)  IoU=85.12 (80.18)  Prec@50=93.75 (89.82)
2023-04-25 22:26:38 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 700/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0461 (0.0446)  IoU=78.00 (80.12)  Prec@50=93.75 (89.81)
2023-04-25 22:27:48 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 800/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0637 (0.0449)  IoU=71.28 (80.08)  Prec@50=81.25 (89.85)
2023-04-25 22:28:58 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [ 900/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0399 (0.0457)  IoU=84.65 (80.14)  Prec@50=93.75 (89.90)
2023-04-25 22:30:07 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [1000/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0309 (0.0460)  IoU=76.54 (80.22)  Prec@50=81.25 (89.93)
2023-04-25 22:31:17 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [1100/1254]  Batch=0.69 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0483 (0.0462)  IoU=74.36 (80.11)  Prec@50=81.25 (89.80)
2023-04-25 22:32:27 | INFO     | utils.misc:106 - Training: Epoch=[7/50] [1200/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0696 (0.0466)  IoU=72.93 (79.95)  Prec@50=81.25 (89.65)
2023-04-25 22:37:44 | INFO     | engine.engine:148 - Evaluation: Epoch=[7/50]  IoU=67.44  Pr@50: 75.77  Pr@60: 68.47  Pr@70: 60.04  Pr@80: 49.35  Pr@90: 31.18  
2023-04-25 22:39:13 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 100/1254]  Batch=0.71 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0526 (0.0408)  IoU=81.34 (80.73)  Prec@50=93.75 (90.25)
2023-04-25 22:40:23 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 200/1254]  Batch=0.71 (0.73)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0413 (0.0415)  IoU=89.01 (81.12)  Prec@50=100.00 (90.53)
2023-04-25 22:41:33 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 300/1254]  Batch=0.79 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0601 (0.0435)  IoU=74.84 (80.60)  Prec@50=87.50 (90.04)
2023-04-25 22:42:43 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 400/1254]  Batch=0.73 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0561 (0.0435)  IoU=77.42 (80.56)  Prec@50=81.25 (90.06)
2023-04-25 22:43:53 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 500/1254]  Batch=0.76 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0556 (0.0436)  IoU=83.61 (80.78)  Prec@50=93.75 (90.31)
2023-04-25 22:45:03 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 600/1254]  Batch=0.61 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0649 (0.0436)  IoU=80.52 (80.98)  Prec@50=93.75 (90.58)
2023-04-25 22:46:13 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 700/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0363 (0.0435)  IoU=77.12 (80.96)  Prec@50=87.50 (90.50)
2023-04-25 22:47:22 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 800/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0407 (0.0430)  IoU=81.37 (80.95)  Prec@50=100.00 (90.64)
2023-04-25 22:48:32 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [ 900/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0477 (0.0436)  IoU=82.87 (80.78)  Prec@50=87.50 (90.44)
2023-04-25 22:49:41 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [1000/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0856 (0.0443)  IoU=80.82 (80.71)  Prec@50=93.75 (90.39)
2023-04-25 22:50:51 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [1100/1254]  Batch=0.63 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0467 (0.0458)  IoU=86.21 (80.41)  Prec@50=93.75 (90.07)
2023-04-25 22:52:01 | INFO     | utils.misc:106 - Training: Epoch=[8/50] [1200/1254]  Batch=0.71 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0297 (0.0460)  IoU=79.22 (80.37)  Prec@50=87.50 (90.00)
2023-04-25 22:57:17 | INFO     | engine.engine:148 - Evaluation: Epoch=[8/50]  IoU=66.48  Pr@50: 74.77  Pr@60: 67.16  Pr@70: 58.37  Pr@80: 48.02  Pr@90: 29.23  
2023-04-25 22:58:40 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 100/1254]  Batch=0.66 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0417 (0.0440)  IoU=76.24 (79.39)  Prec@50=81.25 (88.81)
2023-04-25 22:59:50 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 200/1254]  Batch=0.69 (0.73)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0382 (0.0420)  IoU=84.47 (80.69)  Prec@50=93.75 (90.06)
2023-04-25 23:01:00 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 300/1254]  Batch=0.66 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0329 (0.0414)  IoU=80.19 (81.00)  Prec@50=87.50 (90.50)
2023-04-25 23:02:10 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 400/1254]  Batch=0.71 (0.71)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0384 (0.0405)  IoU=80.21 (81.43)  Prec@50=93.75 (90.88)
2023-04-25 23:03:19 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 500/1254]  Batch=0.64 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0333 (0.0422)  IoU=83.78 (80.98)  Prec@50=93.75 (90.42)
2023-04-25 23:04:27 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 600/1254]  Batch=0.62 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0411 (0.0420)  IoU=85.82 (81.06)  Prec@50=93.75 (90.51)
2023-04-25 23:05:37 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 700/1254]  Batch=0.70 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0342 (0.0419)  IoU=86.43 (81.12)  Prec@50=100.00 (90.58)
2023-04-25 23:06:46 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 800/1254]  Batch=0.71 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0387 (0.0421)  IoU=82.10 (81.00)  Prec@50=93.75 (90.45)
2023-04-25 23:07:55 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [ 900/1254]  Batch=0.67 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0828 (0.0419)  IoU=85.66 (81.27)  Prec@50=100.00 (90.78)
2023-04-25 23:09:05 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [1000/1254]  Batch=0.70 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0380 (0.0423)  IoU=85.99 (81.21)  Prec@50=93.75 (90.67)
2023-04-25 23:10:15 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [1100/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0385 (0.0419)  IoU=80.38 (81.26)  Prec@50=93.75 (90.70)
2023-04-25 23:11:25 | INFO     | utils.misc:106 - Training: Epoch=[9/50] [1200/1254]  Batch=0.64 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0351 (0.0420)  IoU=78.14 (81.30)  Prec@50=87.50 (90.72)
2023-04-25 23:16:41 | INFO     | engine.engine:148 - Evaluation: Epoch=[9/50]  IoU=66.07  Pr@50: 74.87  Pr@60: 66.99  Pr@70: 58.01  Pr@80: 47.53  Pr@90: 28.97  
2023-04-25 23:18:06 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 100/1254]  Batch=0.67 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0295 (0.0387)  IoU=87.52 (82.22)  Prec@50=100.00 (91.19)
2023-04-25 23:19:16 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 200/1254]  Batch=0.75 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0223 (0.0411)  IoU=89.16 (81.64)  Prec@50=100.00 (91.03)
2023-04-25 23:20:25 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 300/1254]  Batch=0.70 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0976 (0.0421)  IoU=84.38 (81.66)  Prec@50=93.75 (90.90)
2023-04-25 23:21:35 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 400/1254]  Batch=0.66 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0472 (0.0426)  IoU=69.17 (81.43)  Prec@50=81.25 (90.66)
2023-04-25 23:22:45 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 500/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0234 (0.0414)  IoU=86.46 (81.67)  Prec@50=100.00 (90.97)
2023-04-25 23:23:54 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 600/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0551 (0.0407)  IoU=89.32 (81.84)  Prec@50=93.75 (91.14)
2023-04-25 23:25:04 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 700/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0434 (0.0404)  IoU=86.02 (81.80)  Prec@50=93.75 (91.08)
2023-04-25 23:26:13 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 800/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0367 (0.0402)  IoU=76.21 (81.84)  Prec@50=87.50 (91.12)
2023-04-25 23:27:23 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [ 900/1254]  Batch=0.64 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0500 (0.0398)  IoU=85.42 (81.94)  Prec@50=93.75 (91.27)
2023-04-25 23:28:33 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [1000/1254]  Batch=0.80 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0348 (0.0393)  IoU=83.05 (82.06)  Prec@50=93.75 (91.33)
2023-04-25 23:29:42 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [1100/1254]  Batch=0.76 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0300 (0.0389)  IoU=86.44 (82.14)  Prec@50=93.75 (91.39)
2023-04-25 23:30:51 | INFO     | utils.misc:106 - Training: Epoch=[10/50] [1200/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0376 (0.0390)  IoU=86.99 (82.11)  Prec@50=93.75 (91.35)
2023-04-25 23:36:07 | INFO     | engine.engine:148 - Evaluation: Epoch=[10/50]  IoU=67.35  Pr@50: 74.44  Pr@60: 67.16  Pr@70: 59.77  Pr@80: 51.29  Pr@90: 32.02  
2023-04-25 23:37:31 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 100/1254]  Batch=0.72 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0314 (0.0365)  IoU=80.59 (83.33)  Prec@50=87.50 (92.31)
2023-04-25 23:38:41 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 200/1254]  Batch=0.74 (0.73)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0382 (0.0350)  IoU=92.77 (83.20)  Prec@50=100.00 (92.00)
2023-04-25 23:39:51 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 300/1254]  Batch=0.63 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0399 (0.0348)  IoU=89.58 (83.55)  Prec@50=100.00 (92.42)
2023-04-25 23:41:01 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 400/1254]  Batch=0.68 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0592 (0.0376)  IoU=72.66 (82.94)  Prec@50=87.50 (91.97)
2023-04-25 23:42:11 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 500/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0388 (0.0373)  IoU=86.94 (82.99)  Prec@50=93.75 (92.17)
2023-04-25 23:43:20 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 600/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0525 (0.0383)  IoU=83.29 (82.72)  Prec@50=93.75 (91.85)
2023-04-25 23:44:30 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 700/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0306 (0.0384)  IoU=86.01 (82.77)  Prec@50=100.00 (92.02)
2023-04-25 23:45:40 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 800/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0376 (0.0389)  IoU=86.74 (82.66)  Prec@50=93.75 (91.92)
2023-04-25 23:46:49 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [ 900/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0425 (0.0388)  IoU=79.31 (82.64)  Prec@50=87.50 (91.97)
2023-04-25 23:47:59 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [1000/1254]  Batch=0.69 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0262 (0.0386)  IoU=88.73 (82.62)  Prec@50=100.00 (91.92)
2023-04-25 23:49:09 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [1100/1254]  Batch=0.73 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0341 (0.0384)  IoU=80.10 (82.69)  Prec@50=81.25 (92.02)
2023-04-25 23:50:18 | INFO     | utils.misc:106 - Training: Epoch=[11/50] [1200/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0339 (0.0382)  IoU=90.62 (82.72)  Prec@50=100.00 (92.06)
2023-04-25 23:55:33 | INFO     | engine.engine:148 - Evaluation: Epoch=[11/50]  IoU=69.14  Pr@50: 76.43  Pr@60: 68.97  Pr@70: 61.33  Pr@80: 52.70  Pr@90: 34.68  
2023-04-25 23:57:02 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 100/1254]  Batch=0.74 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0330 (0.0357)  IoU=82.85 (84.29)  Prec@50=93.75 (93.50)
2023-04-25 23:58:12 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 200/1254]  Batch=0.69 (0.73)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0383 (0.0377)  IoU=81.31 (83.47)  Prec@50=93.75 (92.59)
2023-04-25 23:59:22 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 300/1254]  Batch=0.69 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0527 (0.0378)  IoU=86.59 (83.37)  Prec@50=93.75 (92.77)
2023-04-26 00:00:31 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 400/1254]  Batch=0.66 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0288 (0.0380)  IoU=79.58 (82.96)  Prec@50=87.50 (92.25)
2023-04-26 00:01:41 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 500/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0262 (0.0371)  IoU=93.43 (83.06)  Prec@50=100.00 (92.38)
2023-04-26 00:02:51 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 600/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0237 (0.0367)  IoU=84.79 (83.22)  Prec@50=93.75 (92.56)
2023-04-26 00:04:02 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 700/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0335 (0.0359)  IoU=90.62 (83.31)  Prec@50=100.00 (92.55)
2023-04-26 00:05:12 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 800/1254]  Batch=0.74 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0268 (0.0369)  IoU=82.14 (83.14)  Prec@50=93.75 (92.38)
2023-04-26 00:06:21 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [ 900/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0251 (0.0372)  IoU=72.70 (82.88)  Prec@50=81.25 (92.07)
2023-04-26 00:07:31 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [1000/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0226 (0.0375)  IoU=79.89 (82.78)  Prec@50=87.50 (92.02)
2023-04-26 00:08:41 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [1100/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0328 (0.0374)  IoU=82.50 (82.77)  Prec@50=93.75 (91.98)
2023-04-26 00:09:51 | INFO     | utils.misc:106 - Training: Epoch=[12/50] [1200/1254]  Batch=0.73 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0326 (0.0371)  IoU=88.32 (82.86)  Prec@50=100.00 (92.14)
2023-04-26 00:15:07 | INFO     | engine.engine:148 - Evaluation: Epoch=[12/50]  IoU=67.90  Pr@50: 76.00  Pr@60: 68.33  Pr@70: 59.84  Pr@80: 50.24  Pr@90: 32.68  
2023-04-26 00:16:31 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 100/1254]  Batch=0.70 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0262 (0.0346)  IoU=74.31 (83.64)  Prec@50=87.50 (92.75)
2023-04-26 00:17:40 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 200/1254]  Batch=0.68 (0.73)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0514 (0.0340)  IoU=83.91 (83.97)  Prec@50=87.50 (93.31)
2023-04-26 00:18:50 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 300/1254]  Batch=0.72 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0271 (0.0350)  IoU=83.07 (83.78)  Prec@50=87.50 (92.90)
2023-04-26 00:19:59 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 400/1254]  Batch=0.69 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0285 (0.0346)  IoU=84.01 (83.88)  Prec@50=87.50 (92.92)
2023-04-26 00:21:09 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 500/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0448 (0.0344)  IoU=85.27 (83.90)  Prec@50=100.00 (92.79)
2023-04-26 00:22:19 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 600/1254]  Batch=0.76 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0343 (0.0343)  IoU=85.00 (83.95)  Prec@50=93.75 (92.81)
2023-04-26 00:23:28 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 700/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0263 (0.0342)  IoU=88.46 (83.86)  Prec@50=100.00 (92.71)
2023-04-26 00:24:38 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 800/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0479 (0.0343)  IoU=76.24 (83.87)  Prec@50=87.50 (92.73)
2023-04-26 00:25:47 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [ 900/1254]  Batch=0.71 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0379 (0.0347)  IoU=76.11 (83.85)  Prec@50=81.25 (92.75)
2023-04-26 00:26:57 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [1000/1254]  Batch=0.70 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0445 (0.0345)  IoU=82.70 (83.90)  Prec@50=93.75 (92.83)
2023-04-26 00:28:06 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [1100/1254]  Batch=0.69 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0357 (0.0346)  IoU=86.13 (83.80)  Prec@50=100.00 (92.73)
2023-04-26 00:29:16 | INFO     | utils.misc:106 - Training: Epoch=[13/50] [1200/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0287 (0.0346)  IoU=90.39 (83.81)  Prec@50=93.75 (92.76)
2023-04-26 00:34:32 | INFO     | engine.engine:148 - Evaluation: Epoch=[13/50]  IoU=65.18  Pr@50: 72.20  Pr@60: 64.07  Pr@70: 56.56  Pr@80: 47.72  Pr@90: 28.22  
2023-04-26 00:35:57 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 100/1254]  Batch=0.69 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0421 (0.0414)  IoU=84.64 (81.73)  Prec@50=93.75 (91.19)
2023-04-26 00:37:07 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 200/1254]  Batch=0.69 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0301 (0.0400)  IoU=84.35 (82.29)  Prec@50=100.00 (91.66)
2023-04-26 00:38:17 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 300/1254]  Batch=0.80 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0383 (0.0376)  IoU=83.11 (82.80)  Prec@50=93.75 (92.15)
2023-04-26 00:39:26 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 400/1254]  Batch=0.68 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0334 (0.0360)  IoU=76.88 (83.28)  Prec@50=81.25 (92.50)
2023-04-26 00:40:36 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 500/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0293 (0.0349)  IoU=88.02 (83.59)  Prec@50=100.00 (92.65)
2023-04-26 00:41:45 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 600/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0455 (0.0354)  IoU=83.59 (83.59)  Prec@50=100.00 (92.56)
2023-04-26 00:42:55 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 700/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0300 (0.0351)  IoU=79.52 (83.53)  Prec@50=93.75 (92.49)
2023-04-26 00:44:05 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 800/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0311 (0.0347)  IoU=82.57 (83.55)  Prec@50=87.50 (92.48)
2023-04-26 00:45:15 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [ 900/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0253 (0.0346)  IoU=80.93 (83.56)  Prec@50=87.50 (92.51)
2023-04-26 00:46:24 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [1000/1254]  Batch=0.76 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0216 (0.0345)  IoU=85.49 (83.68)  Prec@50=93.75 (92.67)
2023-04-26 00:47:34 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [1100/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0353 (0.0344)  IoU=82.95 (83.72)  Prec@50=100.00 (92.66)
2023-04-26 00:48:44 | INFO     | utils.misc:106 - Training: Epoch=[14/50] [1200/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0242 (0.0344)  IoU=88.27 (83.78)  Prec@50=100.00 (92.72)
2023-04-26 00:54:02 | INFO     | engine.engine:148 - Evaluation: Epoch=[14/50]  IoU=67.00  Pr@50: 75.32  Pr@60: 67.18  Pr@70: 59.03  Pr@80: 49.37  Pr@90: 32.99  
2023-04-26 00:55:28 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 100/1254]  Batch=0.62 (0.79)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0201 (0.0297)  IoU=82.82 (84.78)  Prec@50=93.75 (93.62)
2023-04-26 00:56:37 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 200/1254]  Batch=0.69 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0251 (0.0304)  IoU=78.90 (84.99)  Prec@50=87.50 (94.03)
2023-04-26 00:57:47 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 300/1254]  Batch=0.67 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0540 (0.0307)  IoU=76.64 (84.84)  Prec@50=81.25 (93.81)
2023-04-26 00:58:53 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 400/1254]  Batch=0.68 (0.71)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0244 (0.0305)  IoU=84.75 (85.02)  Prec@50=93.75 (94.16)
2023-04-26 01:00:14 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 500/1254]  Batch=0.80 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0391 (0.0310)  IoU=85.43 (85.00)  Prec@50=87.50 (94.12)
2023-04-26 01:01:28 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 600/1254]  Batch=0.66 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0377 (0.0312)  IoU=88.34 (85.07)  Prec@50=100.00 (94.15)
2023-04-26 01:02:36 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 700/1254]  Batch=0.77 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0393 (0.0314)  IoU=84.66 (84.92)  Prec@50=93.75 (93.99)
2023-04-26 01:03:44 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 800/1254]  Batch=0.63 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0286 (0.0312)  IoU=78.69 (84.99)  Prec@50=87.50 (94.02)
2023-04-26 01:04:53 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [ 900/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0290 (0.0311)  IoU=90.61 (84.94)  Prec@50=100.00 (93.92)
2023-04-26 01:06:05 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [1000/1254]  Batch=0.82 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0427 (0.0310)  IoU=89.58 (84.93)  Prec@50=100.00 (93.86)
2023-04-26 01:07:23 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [1100/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0324 (0.0310)  IoU=86.58 (84.86)  Prec@50=93.75 (93.78)
2023-04-26 01:08:36 | INFO     | utils.misc:106 - Training: Epoch=[15/50] [1200/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0202 (0.0314)  IoU=75.61 (84.75)  Prec@50=87.50 (93.66)
2023-04-26 01:13:57 | INFO     | engine.engine:148 - Evaluation: Epoch=[15/50]  IoU=36.71  Pr@50: 34.76  Pr@60: 29.82  Pr@70: 25.41  Pr@80: 20.78  Pr@90: 13.68  
2023-04-26 01:15:21 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 100/1254]  Batch=0.72 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0387 (0.0348)  IoU=81.39 (84.24)  Prec@50=87.50 (93.44)
2023-04-26 01:16:33 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 200/1254]  Batch=0.75 (0.75)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0400 (0.0342)  IoU=92.21 (84.38)  Prec@50=100.00 (93.38)
2023-04-26 01:17:43 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 300/1254]  Batch=0.70 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0438 (0.0339)  IoU=81.04 (83.91)  Prec@50=93.75 (92.96)
2023-04-26 01:18:55 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 400/1254]  Batch=0.72 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0225 (0.0332)  IoU=78.94 (84.13)  Prec@50=93.75 (93.14)
2023-04-26 01:20:05 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 500/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0255 (0.0322)  IoU=92.67 (84.41)  Prec@50=100.00 (93.33)
2023-04-26 01:21:17 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 600/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0385 (0.0315)  IoU=81.83 (84.56)  Prec@50=87.50 (93.50)
2023-04-26 01:22:28 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 700/1254]  Batch=0.72 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0326 (0.0312)  IoU=80.75 (84.61)  Prec@50=87.50 (93.40)
2023-04-26 01:23:39 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 800/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0294 (0.0317)  IoU=86.90 (84.51)  Prec@50=93.75 (93.37)
2023-04-26 01:24:49 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [ 900/1254]  Batch=0.73 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0260 (0.0320)  IoU=76.41 (84.46)  Prec@50=87.50 (93.38)
2023-04-26 01:26:00 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [1000/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0190 (0.0319)  IoU=88.52 (84.49)  Prec@50=100.00 (93.39)
2023-04-26 01:27:10 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [1100/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0232 (0.0318)  IoU=82.75 (84.59)  Prec@50=87.50 (93.51)
2023-04-26 01:28:20 | INFO     | utils.misc:106 - Training: Epoch=[16/50] [1200/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0272 (0.0318)  IoU=82.62 (84.58)  Prec@50=93.75 (93.46)
2023-04-26 01:33:41 | INFO     | engine.engine:148 - Evaluation: Epoch=[16/50]  IoU=66.15  Pr@50: 74.21  Pr@60: 66.03  Pr@70: 57.26  Pr@80: 47.43  Pr@90: 30.24  
2023-04-26 01:35:05 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 100/1254]  Batch=0.72 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0292 (0.0336)  IoU=82.59 (84.74)  Prec@50=93.75 (93.81)
2023-04-26 01:36:16 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 200/1254]  Batch=0.74 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0222 (0.0323)  IoU=85.46 (84.71)  Prec@50=93.75 (93.50)
2023-04-26 01:37:27 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 300/1254]  Batch=0.68 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0323 (0.0321)  IoU=92.63 (84.83)  Prec@50=100.00 (93.54)
2023-04-26 01:38:37 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 400/1254]  Batch=0.71 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0551 (0.0316)  IoU=87.52 (84.94)  Prec@50=100.00 (93.52)
2023-04-26 01:39:49 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 500/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0294 (0.0313)  IoU=90.04 (85.01)  Prec@50=100.00 (93.67)
2023-04-26 01:41:01 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 600/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0367 (0.0334)  IoU=89.83 (84.73)  Prec@50=93.75 (93.46)
2023-04-26 01:42:11 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 700/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0450 (0.0330)  IoU=86.95 (84.68)  Prec@50=93.75 (93.41)
2023-04-26 01:43:22 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 800/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0264 (0.0327)  IoU=83.79 (84.53)  Prec@50=93.75 (93.31)
2023-04-26 01:44:32 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [ 900/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0160 (0.0323)  IoU=81.49 (84.51)  Prec@50=87.50 (93.28)
2023-04-26 01:45:43 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [1000/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0215 (0.0321)  IoU=89.19 (84.59)  Prec@50=100.00 (93.35)
2023-04-26 01:46:53 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [1100/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0229 (0.0319)  IoU=83.97 (84.59)  Prec@50=93.75 (93.32)
2023-04-26 01:48:04 | INFO     | utils.misc:106 - Training: Epoch=[17/50] [1200/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0256 (0.0320)  IoU=90.36 (84.61)  Prec@50=100.00 (93.33)
2023-04-26 01:53:23 | INFO     | engine.engine:148 - Evaluation: Epoch=[17/50]  IoU=68.06  Pr@50: 76.52  Pr@60: 69.42  Pr@70: 61.41  Pr@80: 51.35  Pr@90: 33.20  
2023-04-26 01:54:50 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 100/1254]  Batch=0.69 (0.79)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0230 (0.0278)  IoU=77.63 (85.46)  Prec@50=81.25 (94.00)
2023-04-26 01:56:01 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 200/1254]  Batch=0.70 (0.75)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0294 (0.0301)  IoU=85.55 (85.68)  Prec@50=93.75 (94.31)
2023-04-26 01:57:12 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 300/1254]  Batch=0.81 (0.74)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0222 (0.0296)  IoU=83.63 (86.03)  Prec@50=93.75 (94.69)
2023-04-26 01:58:23 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 400/1254]  Batch=0.72 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0286 (0.0293)  IoU=79.89 (86.03)  Prec@50=87.50 (94.78)
2023-04-26 01:59:32 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 500/1254]  Batch=0.82 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0284 (0.0297)  IoU=86.06 (85.68)  Prec@50=93.75 (94.40)
2023-04-26 02:00:42 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 600/1254]  Batch=0.66 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0413 (0.0322)  IoU=89.59 (85.04)  Prec@50=100.00 (93.91)
2023-04-26 02:01:53 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 700/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0287 (0.0329)  IoU=79.22 (84.87)  Prec@50=87.50 (93.79)
2023-04-26 02:03:03 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 800/1254]  Batch=0.62 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0175 (0.0324)  IoU=86.86 (84.87)  Prec@50=93.75 (93.72)
2023-04-26 02:04:13 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [ 900/1254]  Batch=0.77 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0190 (0.0322)  IoU=87.17 (84.89)  Prec@50=93.75 (93.69)
2023-04-26 02:05:23 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [1000/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0215 (0.0318)  IoU=89.69 (84.95)  Prec@50=100.00 (93.72)
2023-04-26 02:06:34 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [1100/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0312 (0.0318)  IoU=78.82 (84.94)  Prec@50=87.50 (93.72)
2023-04-26 02:07:44 | INFO     | utils.misc:106 - Training: Epoch=[18/50] [1200/1254]  Batch=0.66 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0422 (0.0315)  IoU=79.42 (84.88)  Prec@50=93.75 (93.64)
2023-04-26 02:13:08 | INFO     | engine.engine:148 - Evaluation: Epoch=[18/50]  IoU=67.43  Pr@50: 75.82  Pr@60: 68.60  Pr@70: 60.11  Pr@80: 51.49  Pr@90: 35.05  
2023-04-26 02:14:34 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 100/1254]  Batch=0.66 (0.79)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0209 (0.0283)  IoU=82.62 (85.44)  Prec@50=93.75 (93.81)
2023-04-26 02:15:45 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 200/1254]  Batch=0.72 (0.75)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0398 (0.0289)  IoU=77.72 (85.38)  Prec@50=87.50 (93.53)
2023-04-26 02:16:55 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 300/1254]  Batch=0.67 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0235 (0.0299)  IoU=85.04 (85.34)  Prec@50=87.50 (93.60)
2023-04-26 02:18:07 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 400/1254]  Batch=0.78 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0317 (0.0299)  IoU=74.07 (85.39)  Prec@50=81.25 (93.81)
2023-04-26 02:19:17 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 500/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0231 (0.0292)  IoU=89.64 (85.57)  Prec@50=100.00 (94.05)
2023-04-26 02:20:28 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 600/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0309 (0.0293)  IoU=79.86 (85.45)  Prec@50=87.50 (93.97)
2023-04-26 02:21:39 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 700/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0188 (0.0291)  IoU=84.89 (85.61)  Prec@50=93.75 (94.14)
2023-04-26 02:22:49 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 800/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0362 (0.0290)  IoU=77.48 (85.67)  Prec@50=93.75 (94.15)
2023-04-26 02:24:00 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [ 900/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0181 (0.0292)  IoU=91.11 (85.59)  Prec@50=100.00 (94.06)
2023-04-26 02:25:10 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [1000/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0151 (0.0292)  IoU=80.12 (85.48)  Prec@50=93.75 (93.96)
2023-04-26 02:26:20 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [1100/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0173 (0.0290)  IoU=84.55 (85.50)  Prec@50=93.75 (94.00)
2023-04-26 02:27:31 | INFO     | utils.misc:106 - Training: Epoch=[19/50] [1200/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0289 (0.0289)  IoU=86.93 (85.52)  Prec@50=93.75 (94.03)
2023-04-26 02:32:53 | INFO     | engine.engine:148 - Evaluation: Epoch=[19/50]  IoU=66.95  Pr@50: 75.54  Pr@60: 68.08  Pr@70: 59.05  Pr@80: 50.07  Pr@90: 33.12  
2023-04-26 02:34:18 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 100/1254]  Batch=0.68 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0185 (0.0258)  IoU=82.80 (86.10)  Prec@50=93.75 (94.25)
2023-04-26 02:35:27 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 200/1254]  Batch=0.73 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0303 (0.0265)  IoU=85.72 (85.74)  Prec@50=93.75 (94.03)
2023-04-26 02:36:38 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 300/1254]  Batch=0.69 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0286 (0.0270)  IoU=92.93 (86.09)  Prec@50=100.00 (94.40)
2023-04-26 02:37:49 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 400/1254]  Batch=0.79 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0324 (0.0272)  IoU=81.88 (85.87)  Prec@50=87.50 (94.25)
2023-04-26 02:38:59 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 500/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0249 (0.0272)  IoU=87.08 (85.82)  Prec@50=100.00 (94.21)
2023-04-26 02:40:09 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 600/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0395 (0.0278)  IoU=89.65 (85.87)  Prec@50=100.00 (94.32)
2023-04-26 02:41:19 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 700/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0209 (0.0279)  IoU=89.53 (85.79)  Prec@50=100.00 (94.27)
2023-04-26 02:42:30 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 800/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0252 (0.0277)  IoU=86.76 (85.73)  Prec@50=93.75 (94.27)
2023-04-26 02:43:40 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [ 900/1254]  Batch=0.74 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0219 (0.0277)  IoU=85.22 (85.79)  Prec@50=93.75 (94.33)
2023-04-26 02:44:50 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [1000/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0252 (0.0278)  IoU=94.33 (85.84)  Prec@50=100.00 (94.45)
2023-04-26 02:46:00 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [1100/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0372 (0.0279)  IoU=90.30 (85.86)  Prec@50=100.00 (94.43)
2023-04-26 02:47:11 | INFO     | utils.misc:106 - Training: Epoch=[20/50] [1200/1254]  Batch=0.75 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0302 (0.0278)  IoU=90.98 (85.85)  Prec@50=100.00 (94.40)
2023-04-26 02:52:30 | INFO     | engine.engine:148 - Evaluation: Epoch=[20/50]  IoU=68.82  Pr@50: 76.48  Pr@60: 69.82  Pr@70: 61.59  Pr@80: 53.71  Pr@90: 37.15  
2023-04-26 02:53:54 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 100/1254]  Batch=0.72 (0.77)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0310 (0.0274)  IoU=91.81 (85.47)  Prec@50=100.00 (94.25)
2023-04-26 02:55:05 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 200/1254]  Batch=0.68 (0.74)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0229 (0.0277)  IoU=73.38 (85.88)  Prec@50=81.25 (94.59)
2023-04-26 02:56:15 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 300/1254]  Batch=0.63 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0454 (0.0276)  IoU=80.14 (85.75)  Prec@50=87.50 (94.29)
2023-04-26 02:57:25 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 400/1254]  Batch=0.71 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0348 (0.0277)  IoU=84.60 (85.60)  Prec@50=93.75 (94.16)
2023-04-26 02:58:36 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 500/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0653 (0.0277)  IoU=77.49 (85.72)  Prec@50=87.50 (94.29)
2023-04-26 02:59:46 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 600/1254]  Batch=0.74 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0258 (0.0303)  IoU=84.22 (85.40)  Prec@50=93.75 (94.11)
2023-04-26 03:00:56 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 700/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0257 (0.0304)  IoU=88.50 (85.42)  Prec@50=100.00 (94.12)
2023-04-26 03:02:06 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 800/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0281 (0.0300)  IoU=86.57 (85.44)  Prec@50=100.00 (94.14)
2023-04-26 03:03:17 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [ 900/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0191 (0.0297)  IoU=85.27 (85.42)  Prec@50=93.75 (94.08)
2023-04-26 03:04:28 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [1000/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0301 (0.0302)  IoU=90.90 (85.41)  Prec@50=100.00 (94.09)
2023-04-26 03:05:39 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [1100/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0313 (0.0302)  IoU=87.90 (85.45)  Prec@50=93.75 (94.10)
2023-04-26 03:06:49 | INFO     | utils.misc:106 - Training: Epoch=[21/50] [1200/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0340 (0.0300)  IoU=85.97 (85.42)  Prec@50=93.75 (94.10)
2023-04-26 03:12:09 | INFO     | engine.engine:148 - Evaluation: Epoch=[21/50]  IoU=67.01  Pr@50: 75.23  Pr@60: 67.72  Pr@70: 59.23  Pr@80: 50.41  Pr@90: 33.50  
2023-04-26 03:13:34 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 100/1254]  Batch=0.74 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0218 (0.0265)  IoU=85.54 (86.12)  Prec@50=93.75 (94.19)
2023-04-26 03:14:44 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 200/1254]  Batch=0.74 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0181 (0.0268)  IoU=79.20 (86.71)  Prec@50=93.75 (94.88)
2023-04-26 03:15:55 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 300/1254]  Batch=0.73 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0176 (0.0264)  IoU=72.66 (86.61)  Prec@50=75.00 (94.85)
2023-04-26 03:17:05 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 400/1254]  Batch=0.71 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0403 (0.0290)  IoU=69.78 (86.16)  Prec@50=81.25 (94.50)
2023-04-26 03:18:15 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 500/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0256 (0.0303)  IoU=92.32 (85.79)  Prec@50=100.00 (94.40)
2023-04-26 03:19:25 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 600/1254]  Batch=0.63 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0265 (0.0301)  IoU=90.11 (85.76)  Prec@50=100.00 (94.39)
2023-04-26 03:20:35 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 700/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0266 (0.0297)  IoU=91.24 (85.68)  Prec@50=100.00 (94.31)
2023-04-26 03:21:47 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 800/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0264 (0.0298)  IoU=88.67 (85.69)  Prec@50=100.00 (94.31)
2023-04-26 03:22:58 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [ 900/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0294 (0.0296)  IoU=88.55 (85.63)  Prec@50=100.00 (94.28)
2023-04-26 03:24:09 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [1000/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0342 (0.0295)  IoU=86.77 (85.67)  Prec@50=100.00 (94.29)
2023-04-26 03:25:20 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [1100/1254]  Batch=0.77 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0289 (0.0298)  IoU=90.93 (85.61)  Prec@50=100.00 (94.25)
2023-04-26 03:26:31 | INFO     | utils.misc:106 - Training: Epoch=[22/50] [1200/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0217 (0.0297)  IoU=85.43 (85.59)  Prec@50=100.00 (94.24)
2023-04-26 03:31:53 | INFO     | engine.engine:148 - Evaluation: Epoch=[22/50]  IoU=68.39  Pr@50: 76.30  Pr@60: 69.15  Pr@70: 60.99  Pr@80: 51.20  Pr@90: 32.15  
2023-04-26 03:33:18 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 100/1254]  Batch=0.70 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0148 (0.0264)  IoU=91.34 (86.69)  Prec@50=100.00 (94.56)
2023-04-26 03:34:28 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 200/1254]  Batch=0.68 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0184 (0.0259)  IoU=88.64 (87.06)  Prec@50=100.00 (95.09)
2023-04-26 03:35:39 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 300/1254]  Batch=0.70 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0442 (0.0268)  IoU=89.57 (86.56)  Prec@50=100.00 (94.79)
2023-04-26 03:36:48 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 400/1254]  Batch=0.68 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0311 (0.0266)  IoU=81.75 (86.30)  Prec@50=93.75 (94.64)
2023-04-26 03:37:59 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 500/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0357 (0.0272)  IoU=89.87 (86.20)  Prec@50=93.75 (94.60)
2023-04-26 03:39:10 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 600/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0340 (0.0274)  IoU=82.91 (86.13)  Prec@50=93.75 (94.59)
2023-04-26 03:40:21 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 700/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0242 (0.0275)  IoU=85.32 (86.17)  Prec@50=93.75 (94.63)
2023-04-26 03:41:32 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 800/1254]  Batch=0.81 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0212 (0.0278)  IoU=86.75 (86.14)  Prec@50=93.75 (94.59)
2023-04-26 03:42:43 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [ 900/1254]  Batch=0.72 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0250 (0.0281)  IoU=84.01 (86.09)  Prec@50=93.75 (94.52)
2023-04-26 03:43:54 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [1000/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0320 (0.0299)  IoU=90.01 (85.92)  Prec@50=93.75 (94.41)
2023-04-26 03:45:04 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [1100/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0303 (0.0301)  IoU=80.93 (85.84)  Prec@50=81.25 (94.33)
2023-04-26 03:46:15 | INFO     | utils.misc:106 - Training: Epoch=[23/50] [1200/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0229 (0.0298)  IoU=92.95 (85.80)  Prec@50=100.00 (94.32)
2023-04-26 03:52:12 | INFO     | engine.engine:148 - Evaluation: Epoch=[23/50]  IoU=65.23  Pr@50: 74.06  Pr@60: 65.78  Pr@70: 56.30  Pr@80: 46.64  Pr@90: 28.08  
2023-04-26 03:53:36 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 100/1254]  Batch=0.74 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0184 (0.0265)  IoU=80.54 (86.45)  Prec@50=87.50 (94.94)
2023-04-26 03:54:47 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 200/1254]  Batch=0.72 (0.74)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0126 (0.0259)  IoU=93.52 (86.66)  Prec@50=100.00 (95.25)
2023-04-26 03:55:57 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 300/1254]  Batch=0.74 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.1522 (0.0275)  IoU=84.13 (86.60)  Prec@50=93.75 (95.25)
2023-04-26 03:57:09 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 400/1254]  Batch=0.76 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0370 (0.0290)  IoU=88.41 (86.26)  Prec@50=100.00 (95.00)
2023-04-26 03:58:20 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 500/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0381 (0.0284)  IoU=83.41 (86.20)  Prec@50=100.00 (94.99)
2023-04-26 03:59:30 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 600/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0216 (0.0282)  IoU=91.24 (86.29)  Prec@50=100.00 (95.01)
2023-04-26 04:00:41 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 700/1254]  Batch=0.73 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0275 (0.0278)  IoU=87.09 (86.48)  Prec@50=93.75 (95.15)
2023-04-26 04:01:51 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 800/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0202 (0.0275)  IoU=91.87 (86.39)  Prec@50=100.00 (95.11)
2023-04-26 04:03:01 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [ 900/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0261 (0.0273)  IoU=90.45 (86.40)  Prec@50=100.00 (95.02)
2023-04-26 04:04:11 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [1000/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0278 (0.0275)  IoU=83.66 (86.39)  Prec@50=93.75 (94.99)
2023-04-26 04:05:22 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [1100/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0276 (0.0273)  IoU=91.68 (86.35)  Prec@50=100.00 (94.93)
2023-04-26 04:06:32 | INFO     | utils.misc:106 - Training: Epoch=[24/50] [1200/1254]  Batch=0.66 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0156 (0.0271)  IoU=88.30 (86.37)  Prec@50=100.00 (94.93)
2023-04-26 04:12:48 | INFO     | engine.engine:148 - Evaluation: Epoch=[24/50]  IoU=24.54  Pr@50: 21.15  Pr@60: 16.63  Pr@70: 14.31  Pr@80: 12.59  Pr@90: 11.76  
2023-04-26 04:14:13 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 100/1254]  Batch=0.71 (0.79)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0212 (0.0260)  IoU=90.67 (85.80)  Prec@50=100.00 (93.81)
2023-04-26 04:15:24 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 200/1254]  Batch=0.75 (0.75)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0274 (0.0255)  IoU=86.42 (86.40)  Prec@50=93.75 (94.34)
2023-04-26 04:16:35 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 300/1254]  Batch=0.67 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0322 (0.0251)  IoU=84.21 (86.48)  Prec@50=93.75 (94.54)
2023-04-26 04:17:45 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 400/1254]  Batch=0.69 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0277 (0.0257)  IoU=76.37 (86.61)  Prec@50=81.25 (94.69)
2023-04-26 04:18:55 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 500/1254]  Batch=0.63 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0130 (0.0260)  IoU=87.47 (86.46)  Prec@50=93.75 (94.64)
2023-04-26 04:20:05 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 600/1254]  Batch=0.67 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0248 (0.0262)  IoU=87.32 (86.46)  Prec@50=93.75 (94.71)
2023-04-26 04:21:16 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 700/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0239 (0.0268)  IoU=86.34 (86.28)  Prec@50=100.00 (94.62)
2023-04-26 04:22:26 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 800/1254]  Batch=0.64 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0260 (0.0269)  IoU=85.39 (86.21)  Prec@50=87.50 (94.50)
2023-04-26 04:23:37 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [ 900/1254]  Batch=0.77 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0204 (0.0268)  IoU=90.95 (86.25)  Prec@50=100.00 (94.52)
2023-04-26 04:24:47 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [1000/1254]  Batch=0.66 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0225 (0.0267)  IoU=81.28 (86.27)  Prec@50=93.75 (94.56)
2023-04-26 04:25:57 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [1100/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0404 (0.0267)  IoU=80.96 (86.31)  Prec@50=87.50 (94.58)
2023-04-26 04:27:08 | INFO     | utils.misc:106 - Training: Epoch=[25/50] [1200/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0260 (0.0266)  IoU=92.94 (86.21)  Prec@50=100.00 (94.48)
2023-04-26 04:33:17 | INFO     | engine.engine:148 - Evaluation: Epoch=[25/50]  IoU=65.71  Pr@50: 73.43  Pr@60: 66.07  Pr@70: 57.64  Pr@80: 47.79  Pr@90: 32.48  
2023-04-26 04:34:41 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 100/1254]  Batch=0.70 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0238 (0.0243)  IoU=81.04 (86.67)  Prec@50=87.50 (95.38)
2023-04-26 04:35:53 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 200/1254]  Batch=0.71 (0.75)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0314 (0.0250)  IoU=91.34 (87.18)  Prec@50=100.00 (95.81)
2023-04-26 04:37:03 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 300/1254]  Batch=0.74 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0228 (0.0246)  IoU=86.83 (86.99)  Prec@50=93.75 (95.52)
2023-04-26 04:38:14 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 400/1254]  Batch=0.69 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0476 (0.0248)  IoU=72.19 (87.05)  Prec@50=81.25 (95.56)
2023-04-26 04:39:24 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 500/1254]  Batch=0.76 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0246 (0.0253)  IoU=77.66 (86.93)  Prec@50=87.50 (95.42)
2023-04-26 04:40:34 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 600/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0152 (0.0253)  IoU=87.38 (86.80)  Prec@50=93.75 (95.28)
2023-04-26 04:41:44 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 700/1254]  Batch=0.77 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0322 (0.0254)  IoU=92.01 (86.75)  Prec@50=100.00 (95.14)
2023-04-26 04:42:54 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 800/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0289 (0.0252)  IoU=83.72 (86.71)  Prec@50=87.50 (95.04)
2023-04-26 04:44:04 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [ 900/1254]  Batch=0.71 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0280 (0.0252)  IoU=91.58 (86.75)  Prec@50=93.75 (95.07)
2023-04-26 04:45:14 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [1000/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0199 (0.0253)  IoU=93.96 (86.67)  Prec@50=100.00 (94.95)
2023-04-26 04:46:25 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [1100/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0379 (0.0254)  IoU=93.10 (86.62)  Prec@50=100.00 (94.93)
2023-04-26 04:47:36 | INFO     | utils.misc:106 - Training: Epoch=[26/50] [1200/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0267 (0.0255)  IoU=90.07 (86.58)  Prec@50=100.00 (94.87)
2023-04-26 04:52:58 | INFO     | engine.engine:148 - Evaluation: Epoch=[26/50]  IoU=67.26  Pr@50: 75.75  Pr@60: 68.96  Pr@70: 59.73  Pr@80: 48.96  Pr@90: 31.31  
2023-04-26 04:54:22 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 100/1254]  Batch=0.74 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0286 (0.0260)  IoU=88.81 (87.03)  Prec@50=100.00 (94.75)
2023-04-26 04:55:32 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 200/1254]  Batch=0.76 (0.74)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0178 (0.0258)  IoU=86.55 (86.86)  Prec@50=93.75 (94.53)
2023-04-26 04:56:43 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 300/1254]  Batch=0.68 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0149 (0.0259)  IoU=89.96 (86.37)  Prec@50=100.00 (94.35)
2023-04-26 04:57:53 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 400/1254]  Batch=0.69 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0236 (0.0256)  IoU=83.04 (86.42)  Prec@50=93.75 (94.45)
2023-04-26 04:59:03 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 500/1254]  Batch=0.66 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0262 (0.0255)  IoU=80.77 (86.52)  Prec@50=87.50 (94.58)
2023-04-26 05:00:13 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 600/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0290 (0.0252)  IoU=89.59 (86.58)  Prec@50=100.00 (94.69)
2023-04-26 05:01:24 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 700/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0225 (0.0251)  IoU=93.16 (86.67)  Prec@50=100.00 (94.77)
2023-04-26 05:02:35 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 800/1254]  Batch=0.78 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0216 (0.0255)  IoU=85.99 (86.52)  Prec@50=93.75 (94.66)
2023-04-26 05:03:46 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [ 900/1254]  Batch=0.69 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0390 (0.0258)  IoU=86.28 (86.37)  Prec@50=93.75 (94.52)
2023-04-26 05:04:57 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [1000/1254]  Batch=0.72 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0388 (0.0258)  IoU=86.89 (86.35)  Prec@50=93.75 (94.47)
2023-04-26 05:06:09 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [1100/1254]  Batch=0.80 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0233 (0.0258)  IoU=74.47 (86.28)  Prec@50=81.25 (94.42)
2023-04-26 05:07:19 | INFO     | utils.misc:106 - Training: Epoch=[27/50] [1200/1254]  Batch=0.80 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0228 (0.0259)  IoU=87.97 (86.32)  Prec@50=93.75 (94.44)
2023-04-26 05:12:58 | INFO     | engine.engine:148 - Evaluation: Epoch=[27/50]  IoU=68.09  Pr@50: 76.34  Pr@60: 69.36  Pr@70: 60.50  Pr@80: 51.83  Pr@90: 34.77  
2023-04-26 05:14:26 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 100/1254]  Batch=0.66 (0.81)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0193 (0.0247)  IoU=81.53 (87.15)  Prec@50=93.75 (95.00)
2023-04-26 05:15:40 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 200/1254]  Batch=0.72 (0.78)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0193 (0.0244)  IoU=90.73 (87.07)  Prec@50=100.00 (95.12)
2023-04-26 05:16:50 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 300/1254]  Batch=0.69 (0.75)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0304 (0.0256)  IoU=81.60 (86.69)  Prec@50=87.50 (94.79)
2023-04-26 05:18:02 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 400/1254]  Batch=0.85 (0.74)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0221 (0.0257)  IoU=84.30 (86.53)  Prec@50=93.75 (94.77)
2023-04-26 05:19:15 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 500/1254]  Batch=0.73 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0199 (0.0254)  IoU=83.14 (86.59)  Prec@50=93.75 (94.76)
2023-04-26 05:20:25 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 600/1254]  Batch=0.72 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0227 (0.0259)  IoU=85.93 (86.47)  Prec@50=93.75 (94.64)
2023-04-26 05:21:37 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 700/1254]  Batch=0.74 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0275 (0.0260)  IoU=85.42 (86.52)  Prec@50=93.75 (94.73)
2023-04-26 05:22:49 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 800/1254]  Batch=0.72 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0228 (0.0259)  IoU=92.13 (86.39)  Prec@50=100.00 (94.54)
2023-04-26 05:23:59 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [ 900/1254]  Batch=0.67 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0318 (0.0260)  IoU=86.31 (86.42)  Prec@50=100.00 (94.57)
2023-04-26 05:25:10 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [1000/1254]  Batch=0.72 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0294 (0.0262)  IoU=90.11 (86.30)  Prec@50=100.00 (94.43)
2023-04-26 05:26:21 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [1100/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0270 (0.0261)  IoU=91.68 (86.32)  Prec@50=100.00 (94.47)
2023-04-26 05:27:32 | INFO     | utils.misc:106 - Training: Epoch=[28/50] [1200/1254]  Batch=0.76 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0204 (0.0260)  IoU=86.59 (86.28)  Prec@50=87.50 (94.42)
2023-04-26 05:32:54 | INFO     | engine.engine:148 - Evaluation: Epoch=[28/50]  IoU=68.29  Pr@50: 76.16  Pr@60: 68.77  Pr@70: 59.70  Pr@80: 50.25  Pr@90: 33.67  
2023-04-26 05:34:18 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 100/1254]  Batch=0.66 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0247 (0.0261)  IoU=91.30 (86.17)  Prec@50=100.00 (94.50)
2023-04-26 05:35:28 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 200/1254]  Batch=0.65 (0.74)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0278 (0.0248)  IoU=85.15 (86.89)  Prec@50=93.75 (95.09)
2023-04-26 05:36:39 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 300/1254]  Batch=0.64 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0408 (0.0252)  IoU=81.63 (86.72)  Prec@50=87.50 (95.02)
2023-04-26 05:37:50 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 400/1254]  Batch=0.71 (0.72)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0280 (0.0253)  IoU=82.94 (86.69)  Prec@50=87.50 (94.91)
2023-04-26 05:39:01 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 500/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0222 (0.0252)  IoU=87.92 (86.85)  Prec@50=100.00 (95.03)
2023-04-26 05:40:12 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 600/1254]  Batch=0.73 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0160 (0.0256)  IoU=92.62 (86.72)  Prec@50=100.00 (94.89)
2023-04-26 05:41:23 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 700/1254]  Batch=0.66 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0197 (0.0256)  IoU=84.12 (86.72)  Prec@50=87.50 (94.96)
2023-04-26 05:42:34 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 800/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0210 (0.0255)  IoU=88.28 (86.76)  Prec@50=93.75 (95.06)
2023-04-26 05:43:44 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [ 900/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0168 (0.0254)  IoU=83.44 (86.73)  Prec@50=93.75 (94.98)
2023-04-26 05:44:54 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [1000/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0181 (0.0256)  IoU=90.80 (86.71)  Prec@50=100.00 (94.92)
2023-04-26 05:46:04 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [1100/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0207 (0.0255)  IoU=88.56 (86.63)  Prec@50=100.00 (94.88)
2023-04-26 05:47:16 | INFO     | utils.misc:106 - Training: Epoch=[29/50] [1200/1254]  Batch=0.64 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0207 (0.0256)  IoU=83.39 (86.61)  Prec@50=100.00 (94.91)
2023-04-26 05:52:37 | INFO     | engine.engine:148 - Evaluation: Epoch=[29/50]  IoU=67.49  Pr@50: 75.43  Pr@60: 68.39  Pr@70: 59.64  Pr@80: 50.86  Pr@90: 35.14  
2023-04-26 05:54:02 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 100/1254]  Batch=0.74 (0.78)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0224 (0.0231)  IoU=83.05 (87.16)  Prec@50=87.50 (95.19)
2023-04-26 05:55:14 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 200/1254]  Batch=0.72 (0.75)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0243 (0.0235)  IoU=89.76 (87.27)  Prec@50=100.00 (95.56)
2023-04-26 05:56:25 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 300/1254]  Batch=0.72 (0.74)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0300 (0.0242)  IoU=88.93 (87.06)  Prec@50=93.75 (95.21)
2023-04-26 05:57:36 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 400/1254]  Batch=0.73 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0251 (0.0242)  IoU=82.99 (87.17)  Prec@50=93.75 (95.36)
2023-04-26 05:58:47 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 500/1254]  Batch=0.69 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0502 (0.0242)  IoU=86.41 (87.28)  Prec@50=100.00 (95.50)
2023-04-26 05:59:57 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 600/1254]  Batch=0.65 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0182 (0.0244)  IoU=86.89 (87.10)  Prec@50=93.75 (95.29)
2023-04-26 06:01:07 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 700/1254]  Batch=0.73 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0181 (0.0245)  IoU=86.20 (87.01)  Prec@50=93.75 (95.25)
2023-04-26 06:02:19 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 800/1254]  Batch=0.73 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0272 (0.0244)  IoU=80.58 (86.94)  Prec@50=87.50 (95.13)
2023-04-26 06:03:29 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [ 900/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0246 (0.0244)  IoU=92.18 (86.97)  Prec@50=100.00 (95.17)
2023-04-26 06:04:40 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [1000/1254]  Batch=0.76 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0249 (0.0242)  IoU=87.19 (87.01)  Prec@50=93.75 (95.19)
2023-04-26 06:05:50 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [1100/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0282 (0.0244)  IoU=83.07 (86.95)  Prec@50=93.75 (95.12)
2023-04-26 06:07:01 | INFO     | utils.misc:106 - Training: Epoch=[30/50] [1200/1254]  Batch=0.82 (0.71)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0292 (0.0245)  IoU=89.78 (86.96)  Prec@50=100.00 (95.15)
2023-04-26 06:12:19 | INFO     | engine.engine:148 - Evaluation: Epoch=[30/50]  IoU=65.21  Pr@50: 73.57  Pr@60: 65.05  Pr@70: 56.55  Pr@80: 47.58  Pr@90: 31.35  
2023-04-26 06:13:45 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 100/1254]  Batch=0.69 (0.79)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0213 (0.0237)  IoU=92.33 (87.75)  Prec@50=100.00 (96.06)
2023-04-26 06:14:56 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 200/1254]  Batch=0.71 (0.75)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0213 (0.0235)  IoU=91.83 (87.95)  Prec@50=100.00 (96.25)
2023-04-26 06:16:07 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 300/1254]  Batch=0.72 (0.74)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0279 (0.0235)  IoU=79.61 (87.55)  Prec@50=87.50 (95.60)
2023-04-26 06:17:18 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 400/1254]  Batch=0.68 (0.73)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0294 (0.0235)  IoU=74.33 (87.47)  Prec@50=81.25 (95.47)
2023-04-26 06:18:29 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 500/1254]  Batch=0.69 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0202 (0.0232)  IoU=90.40 (87.42)  Prec@50=100.00 (95.46)
2023-04-26 06:19:39 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 600/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0229 (0.0231)  IoU=88.31 (87.57)  Prec@50=93.75 (95.60)
2023-04-26 06:20:50 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 700/1254]  Batch=0.65 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0286 (0.0232)  IoU=92.70 (87.52)  Prec@50=100.00 (95.56)
2023-04-26 06:22:02 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 800/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0204 (0.0231)  IoU=84.35 (87.48)  Prec@50=87.50 (95.48)
2023-04-26 06:23:16 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [ 900/1254]  Batch=0.72 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0359 (0.0238)  IoU=77.85 (87.36)  Prec@50=81.25 (95.32)
2023-04-26 06:24:26 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [1000/1254]  Batch=0.68 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0223 (0.0242)  IoU=84.93 (87.17)  Prec@50=93.75 (95.12)
2023-04-26 06:25:39 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [1100/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0346 (0.0245)  IoU=89.81 (87.12)  Prec@50=100.00 (95.12)
2023-04-26 06:26:51 | INFO     | utils.misc:106 - Training: Epoch=[31/50] [1200/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0211 (0.0249)  IoU=89.63 (87.00)  Prec@50=100.00 (95.04)
2023-04-26 06:32:48 | INFO     | engine.engine:148 - Evaluation: Epoch=[31/50]  IoU=67.25  Pr@50: 75.74  Pr@60: 69.20  Pr@70: 60.58  Pr@80: 51.51  Pr@90: 33.44  
2023-04-26 06:34:14 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 100/1254]  Batch=0.69 (0.80)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0421 (0.0254)  IoU=87.99 (85.52)  Prec@50=93.75 (93.56)
2023-04-26 06:35:24 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 200/1254]  Batch=0.67 (0.75)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0259 (0.0245)  IoU=85.53 (86.16)  Prec@50=93.75 (94.28)
2023-04-26 06:36:38 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 300/1254]  Batch=0.73 (0.74)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0273 (0.0248)  IoU=85.03 (86.33)  Prec@50=93.75 (94.44)
2023-04-26 06:37:49 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 400/1254]  Batch=0.67 (0.74)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0323 (0.0245)  IoU=80.92 (86.64)  Prec@50=87.50 (94.78)
2023-04-26 06:39:00 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 500/1254]  Batch=0.69 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0246 (0.0241)  IoU=79.72 (86.89)  Prec@50=93.75 (94.95)
2023-04-26 06:40:10 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 600/1254]  Batch=0.76 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0274 (0.0241)  IoU=93.20 (86.97)  Prec@50=100.00 (95.03)
2023-04-26 06:41:24 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 700/1254]  Batch=0.66 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0306 (0.0240)  IoU=90.08 (86.99)  Prec@50=100.00 (94.98)
2023-04-26 06:42:36 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 800/1254]  Batch=0.70 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0257 (0.0245)  IoU=90.86 (86.93)  Prec@50=100.00 (95.02)
2023-04-26 06:43:47 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [ 900/1254]  Batch=0.73 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0231 (0.0245)  IoU=88.68 (87.00)  Prec@50=93.75 (95.11)
2023-04-26 06:44:59 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [1000/1254]  Batch=0.74 (0.72)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0257 (0.0248)  IoU=84.14 (87.02)  Prec@50=100.00 (95.16)
2023-04-26 06:46:14 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [1100/1254]  Batch=0.80 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0228 (0.0248)  IoU=88.72 (86.98)  Prec@50=93.75 (95.10)
2023-04-26 06:47:27 | INFO     | utils.misc:106 - Training: Epoch=[32/50] [1200/1254]  Batch=0.76 (0.73)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0313 (0.0248)  IoU=88.07 (86.94)  Prec@50=100.00 (95.08)
2023-04-26 06:52:49 | INFO     | engine.engine:148 - Evaluation: Epoch=[32/50]  IoU=66.84  Pr@50: 74.74  Pr@60: 67.61  Pr@70: 59.10  Pr@80: 50.08  Pr@90: 35.04  
2023-04-26 06:54:17 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 100/1254]  Batch=0.92 (0.81)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0229 (0.0238)  IoU=78.81 (87.16)  Prec@50=87.50 (94.69)
2023-04-26 06:55:31 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 200/1254]  Batch=0.90 (0.78)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0206 (0.0231)  IoU=88.40 (87.45)  Prec@50=93.75 (95.38)
2023-04-26 06:56:46 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 300/1254]  Batch=0.77 (0.77)  Data=0.00 (0.03)  Lr=0.000100  Loss=0.0277 (0.0227)  IoU=85.15 (87.31)  Prec@50=93.75 (95.06)
2023-04-26 06:57:59 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 400/1254]  Batch=0.69 (0.76)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0142 (0.0231)  IoU=78.17 (87.17)  Prec@50=87.50 (95.08)
2023-04-26 06:59:12 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 500/1254]  Batch=0.66 (0.75)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0149 (0.0233)  IoU=93.17 (87.12)  Prec@50=100.00 (94.99)
2023-04-26 07:00:25 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 600/1254]  Batch=0.69 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0185 (0.0231)  IoU=91.02 (87.32)  Prec@50=100.00 (95.22)
2023-04-26 07:01:39 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 700/1254]  Batch=0.70 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0302 (0.0230)  IoU=86.72 (87.31)  Prec@50=100.00 (95.21)
2023-04-26 07:02:55 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 800/1254]  Batch=0.74 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0235 (0.0231)  IoU=86.50 (87.25)  Prec@50=93.75 (95.12)
2023-04-26 07:04:10 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [ 900/1254]  Batch=0.70 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0271 (0.0233)  IoU=84.10 (87.24)  Prec@50=93.75 (95.10)
2023-04-26 07:05:25 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [1000/1254]  Batch=0.72 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0188 (0.0232)  IoU=85.70 (87.23)  Prec@50=87.50 (95.11)
2023-04-26 07:06:41 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [1100/1254]  Batch=0.77 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0184 (0.0233)  IoU=91.28 (87.20)  Prec@50=100.00 (95.10)
2023-04-26 07:07:52 | INFO     | utils.misc:106 - Training: Epoch=[33/50] [1200/1254]  Batch=0.74 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0256 (0.0234)  IoU=85.34 (87.18)  Prec@50=93.75 (95.08)
2023-04-26 07:13:17 | INFO     | engine.engine:148 - Evaluation: Epoch=[33/50]  IoU=65.94  Pr@50: 74.21  Pr@60: 67.01  Pr@70: 58.36  Pr@80: 49.33  Pr@90: 35.01  
2023-04-26 07:14:48 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 100/1254]  Batch=0.75 (0.84)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0247 (0.0238)  IoU=93.01 (87.39)  Prec@50=100.00 (95.50)
2023-04-26 07:16:00 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 200/1254]  Batch=0.74 (0.78)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0156 (0.0235)  IoU=90.89 (87.44)  Prec@50=100.00 (95.59)
2023-04-26 07:17:12 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 300/1254]  Batch=0.80 (0.76)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0172 (0.0237)  IoU=90.95 (87.44)  Prec@50=93.75 (95.48)
2023-04-26 07:18:24 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 400/1254]  Batch=0.70 (0.75)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0230 (0.0236)  IoU=89.27 (87.61)  Prec@50=100.00 (95.64)
2023-04-26 07:19:36 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 500/1254]  Batch=0.73 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0286 (0.0232)  IoU=82.33 (87.70)  Prec@50=93.75 (95.69)
2023-04-26 07:20:48 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 600/1254]  Batch=0.76 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0171 (0.0229)  IoU=91.41 (87.70)  Prec@50=100.00 (95.69)
2023-04-26 07:22:04 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 700/1254]  Batch=0.69 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0229 (0.0229)  IoU=82.70 (87.69)  Prec@50=93.75 (95.68)
2023-04-26 07:23:19 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 800/1254]  Batch=0.73 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0176 (0.0228)  IoU=93.48 (87.61)  Prec@50=100.00 (95.64)
2023-04-26 07:24:34 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [ 900/1254]  Batch=0.67 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0144 (0.0228)  IoU=88.54 (87.61)  Prec@50=93.75 (95.62)
2023-04-26 07:25:46 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [1000/1254]  Batch=0.70 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0214 (0.0232)  IoU=81.61 (87.43)  Prec@50=87.50 (95.48)
2023-04-26 07:26:57 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [1100/1254]  Batch=0.71 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0298 (0.0233)  IoU=93.44 (87.35)  Prec@50=100.00 (95.42)
2023-04-26 07:28:11 | INFO     | utils.misc:106 - Training: Epoch=[34/50] [1200/1254]  Batch=0.73 (0.74)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0263 (0.0234)  IoU=92.44 (87.26)  Prec@50=100.00 (95.38)
2023-04-26 07:33:56 | INFO     | engine.engine:148 - Evaluation: Epoch=[34/50]  IoU=67.51  Pr@50: 75.66  Pr@60: 68.95  Pr@70: 60.75  Pr@80: 50.75  Pr@90: 31.18  
2023-04-26 07:35:24 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 100/1254]  Batch=0.88 (0.81)  Data=0.00 (0.07)  Lr=0.000100  Loss=0.0211 (0.0326)  IoU=92.36 (87.05)  Prec@50=100.00 (95.38)
2023-04-26 07:36:38 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 200/1254]  Batch=0.68 (0.78)  Data=0.00 (0.04)  Lr=0.000100  Loss=0.0262 (0.0287)  IoU=89.39 (86.91)  Prec@50=100.00 (95.22)
2023-04-26 07:37:54 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 300/1254]  Batch=0.71 (0.77)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0285 (0.0263)  IoU=82.43 (87.24)  Prec@50=87.50 (95.40)
2023-04-26 07:39:07 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 400/1254]  Batch=0.65 (0.76)  Data=0.00 (0.02)  Lr=0.000100  Loss=0.0354 (0.0265)  IoU=87.68 (87.14)  Prec@50=93.75 (95.16)
2023-04-26 07:40:19 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 500/1254]  Batch=0.63 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0175 (0.0258)  IoU=87.15 (87.23)  Prec@50=93.75 (95.29)
2023-04-26 07:41:33 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 600/1254]  Batch=1.34 (0.75)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0224 (0.0258)  IoU=85.23 (87.04)  Prec@50=93.75 (95.17)
2023-04-26 07:42:56 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 700/1254]  Batch=0.65 (0.76)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0183 (0.0255)  IoU=85.87 (87.05)  Prec@50=93.75 (95.15)
2023-04-26 07:45:37 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 800/1254]  Batch=0.64 (0.87)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0334 (0.0251)  IoU=92.33 (87.07)  Prec@50=100.00 (95.20)
2023-04-26 07:47:31 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [ 900/1254]  Batch=0.64 (0.90)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0137 (0.0250)  IoU=88.93 (87.09)  Prec@50=93.75 (95.17)
2023-04-26 07:49:11 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [1000/1254]  Batch=0.71 (0.91)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0217 (0.0250)  IoU=91.39 (87.07)  Prec@50=100.00 (95.15)
2023-04-26 07:50:27 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [1100/1254]  Batch=0.73 (0.89)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0168 (0.0247)  IoU=85.91 (87.11)  Prec@50=100.00 (95.18)
2023-04-26 07:51:42 | INFO     | utils.misc:106 - Training: Epoch=[35/50] [1200/1254]  Batch=0.69 (0.88)  Data=0.00 (0.01)  Lr=0.000100  Loss=0.0312 (0.0245)  IoU=83.33 (87.16)  Prec@50=87.50 (95.21)
2023-04-26 07:57:07 | INFO     | engine.engine:148 - Evaluation: Epoch=[35/50]  IoU=68.12  Pr@50: 76.48  Pr@60: 69.88  Pr@70: 61.61  Pr@80: 53.78  Pr@90: 37.83  
2023-04-26 07:58:33 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 100/1254]  Batch=0.73 (0.80)  Data=0.00 (0.08)  Lr=0.000010  Loss=0.0183 (0.0204)  IoU=93.07 (87.32)  Prec@50=100.00 (94.75)
2023-04-26 07:59:45 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 200/1254]  Batch=0.77 (0.76)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0271 (0.0210)  IoU=90.07 (87.75)  Prec@50=100.00 (95.28)
2023-04-26 08:00:57 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 300/1254]  Batch=0.69 (0.74)  Data=0.00 (0.03)  Lr=0.000010  Loss=0.0268 (0.0212)  IoU=84.18 (87.65)  Prec@50=87.50 (95.23)
2023-04-26 08:02:09 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 400/1254]  Batch=0.75 (0.74)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0205 (0.0213)  IoU=78.72 (87.74)  Prec@50=87.50 (95.27)
2023-04-26 08:03:20 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 500/1254]  Batch=0.72 (0.73)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0217 (0.0212)  IoU=93.93 (87.95)  Prec@50=100.00 (95.55)
2023-04-26 08:04:31 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 600/1254]  Batch=0.75 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0278 (0.0211)  IoU=91.93 (88.07)  Prec@50=100.00 (95.66)
2023-04-26 08:05:46 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 700/1254]  Batch=0.61 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0208 (0.0210)  IoU=94.46 (88.00)  Prec@50=100.00 (95.58)
2023-04-26 08:06:58 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 800/1254]  Batch=0.66 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0286 (0.0210)  IoU=85.74 (88.03)  Prec@50=87.50 (95.62)
2023-04-26 08:08:08 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [ 900/1254]  Batch=0.66 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0210 (0.0209)  IoU=79.62 (88.08)  Prec@50=81.25 (95.66)
2023-04-26 08:09:18 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [1000/1254]  Batch=0.63 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0227 (0.0209)  IoU=86.25 (88.08)  Prec@50=100.00 (95.65)
2023-04-26 08:10:26 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [1100/1254]  Batch=0.71 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0137 (0.0208)  IoU=73.24 (88.10)  Prec@50=81.25 (95.66)
2023-04-26 08:11:38 | INFO     | utils.misc:106 - Training: Epoch=[36/50] [1200/1254]  Batch=0.66 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0191 (0.0209)  IoU=86.72 (88.11)  Prec@50=93.75 (95.68)
2023-04-26 08:17:00 | INFO     | engine.engine:148 - Evaluation: Epoch=[36/50]  IoU=68.88  Pr@50: 77.03  Pr@60: 70.44  Pr@70: 61.97  Pr@80: 53.86  Pr@90: 37.73  
2023-04-26 08:18:33 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 100/1254]  Batch=0.72 (0.86)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0236 (0.0207)  IoU=86.52 (87.63)  Prec@50=93.75 (95.62)
2023-04-26 08:19:56 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 200/1254]  Batch=0.70 (0.85)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0171 (0.0205)  IoU=93.93 (88.39)  Prec@50=100.00 (96.25)
2023-04-26 08:21:10 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 300/1254]  Batch=0.68 (0.81)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0205 (0.0203)  IoU=85.86 (88.45)  Prec@50=93.75 (96.19)
2023-04-26 08:22:27 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 400/1254]  Batch=0.71 (0.80)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0238 (0.0207)  IoU=87.34 (88.34)  Prec@50=93.75 (96.06)
2023-04-26 08:23:39 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 500/1254]  Batch=0.70 (0.78)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0211 (0.0205)  IoU=91.91 (88.41)  Prec@50=100.00 (95.96)
2023-04-26 08:24:46 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 600/1254]  Batch=0.70 (0.77)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0188 (0.0207)  IoU=95.39 (88.41)  Prec@50=100.00 (95.91)
2023-04-26 08:26:06 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 700/1254]  Batch=0.65 (0.77)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0178 (0.0207)  IoU=83.29 (88.46)  Prec@50=93.75 (96.01)
2023-04-26 08:27:23 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 800/1254]  Batch=0.68 (0.77)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0288 (0.0207)  IoU=86.69 (88.40)  Prec@50=93.75 (95.95)
2023-04-26 08:28:36 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [ 900/1254]  Batch=0.66 (0.77)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0171 (0.0207)  IoU=81.99 (88.38)  Prec@50=87.50 (95.96)
2023-04-26 08:29:47 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [1000/1254]  Batch=0.69 (0.76)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0171 (0.0206)  IoU=84.53 (88.33)  Prec@50=93.75 (95.92)
2023-04-26 08:31:12 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [1100/1254]  Batch=0.81 (0.77)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0175 (0.0206)  IoU=92.91 (88.30)  Prec@50=100.00 (95.87)
2023-04-26 08:32:40 | INFO     | utils.misc:106 - Training: Epoch=[37/50] [1200/1254]  Batch=0.76 (0.78)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0161 (0.0206)  IoU=75.57 (88.27)  Prec@50=81.25 (95.81)
2023-04-26 08:38:02 | INFO     | engine.engine:148 - Evaluation: Epoch=[37/50]  IoU=68.62  Pr@50: 76.92  Pr@60: 70.36  Pr@70: 62.20  Pr@80: 53.96  Pr@90: 37.88  
2023-04-26 08:39:25 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 100/1254]  Batch=0.67 (0.77)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0216 (0.0200)  IoU=94.54 (88.16)  Prec@50=100.00 (95.88)
2023-04-26 08:40:35 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 200/1254]  Batch=0.79 (0.73)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0198 (0.0205)  IoU=86.59 (88.58)  Prec@50=93.75 (96.09)
2023-04-26 08:41:46 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 300/1254]  Batch=0.74 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0353 (0.0207)  IoU=91.41 (88.46)  Prec@50=100.00 (96.08)
2023-04-26 08:42:54 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 400/1254]  Batch=0.66 (0.71)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0268 (0.0207)  IoU=85.89 (88.44)  Prec@50=93.75 (96.05)
2023-04-26 08:44:05 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 500/1254]  Batch=0.79 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0191 (0.0206)  IoU=85.04 (88.66)  Prec@50=93.75 (96.25)
2023-04-26 08:45:20 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 600/1254]  Batch=0.61 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0220 (0.0204)  IoU=84.44 (88.64)  Prec@50=93.75 (96.22)
2023-04-26 08:46:30 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 700/1254]  Batch=0.66 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0164 (0.0202)  IoU=94.57 (88.55)  Prec@50=100.00 (96.13)
2023-04-26 08:47:45 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 800/1254]  Batch=0.76 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0208 (0.0201)  IoU=92.43 (88.60)  Prec@50=100.00 (96.14)
2023-04-26 08:48:58 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [ 900/1254]  Batch=0.66 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0305 (0.0201)  IoU=90.49 (88.55)  Prec@50=100.00 (96.10)
2023-04-26 08:50:11 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [1000/1254]  Batch=0.67 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0220 (0.0200)  IoU=91.44 (88.55)  Prec@50=100.00 (96.11)
2023-04-26 08:51:20 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [1100/1254]  Batch=0.69 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0195 (0.0201)  IoU=89.63 (88.52)  Prec@50=100.00 (96.11)
2023-04-26 08:52:33 | INFO     | utils.misc:106 - Training: Epoch=[38/50] [1200/1254]  Batch=0.62 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0221 (0.0201)  IoU=89.51 (88.48)  Prec@50=93.75 (96.08)
2023-04-26 08:57:57 | INFO     | engine.engine:148 - Evaluation: Epoch=[38/50]  IoU=68.95  Pr@50: 76.98  Pr@60: 70.64  Pr@70: 62.55  Pr@80: 54.53  Pr@90: 38.56  
2023-04-26 08:59:21 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 100/1254]  Batch=0.66 (0.77)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0193 (0.0191)  IoU=90.45 (87.70)  Prec@50=100.00 (94.88)
2023-04-26 09:00:32 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 200/1254]  Batch=0.62 (0.74)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0189 (0.0195)  IoU=81.15 (87.87)  Prec@50=87.50 (95.06)
2023-04-26 09:01:41 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 300/1254]  Batch=0.66 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0180 (0.0197)  IoU=93.00 (88.23)  Prec@50=100.00 (95.52)
2023-04-26 09:02:51 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 400/1254]  Batch=0.83 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0196 (0.0199)  IoU=85.41 (88.49)  Prec@50=93.75 (95.86)
2023-04-26 09:03:59 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 500/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0212 (0.0199)  IoU=87.06 (88.36)  Prec@50=93.75 (95.90)
2023-04-26 09:05:11 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 600/1254]  Batch=0.75 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0194 (0.0198)  IoU=92.14 (88.43)  Prec@50=100.00 (95.97)
2023-04-26 09:06:23 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 700/1254]  Batch=0.80 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0265 (0.0198)  IoU=93.89 (88.38)  Prec@50=100.00 (95.89)
2023-04-26 09:07:35 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 800/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0178 (0.0197)  IoU=90.66 (88.38)  Prec@50=100.00 (95.91)
2023-04-26 09:08:46 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [ 900/1254]  Batch=0.64 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0209 (0.0197)  IoU=85.25 (88.33)  Prec@50=93.75 (95.83)
2023-04-26 09:09:56 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [1000/1254]  Batch=0.65 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0214 (0.0197)  IoU=91.85 (88.36)  Prec@50=100.00 (95.89)
2023-04-26 09:11:05 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [1100/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0157 (0.0197)  IoU=92.59 (88.37)  Prec@50=100.00 (95.88)
2023-04-26 09:12:14 | INFO     | utils.misc:106 - Training: Epoch=[39/50] [1200/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0224 (0.0197)  IoU=89.30 (88.38)  Prec@50=93.75 (95.86)
2023-04-26 09:17:38 | INFO     | engine.engine:148 - Evaluation: Epoch=[39/50]  IoU=68.71  Pr@50: 76.87  Pr@60: 70.35  Pr@70: 62.09  Pr@80: 53.52  Pr@90: 37.20  
2023-04-26 09:19:02 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 100/1254]  Batch=0.68 (0.76)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0150 (0.0188)  IoU=87.61 (88.40)  Prec@50=93.75 (95.88)
2023-04-26 09:20:17 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 200/1254]  Batch=0.66 (0.76)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0341 (0.0192)  IoU=90.15 (88.96)  Prec@50=100.00 (96.50)
2023-04-26 09:21:30 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 300/1254]  Batch=0.66 (0.75)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0208 (0.0194)  IoU=85.40 (88.63)  Prec@50=93.75 (96.12)
2023-04-26 09:22:43 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 400/1254]  Batch=0.74 (0.74)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0203 (0.0194)  IoU=84.03 (88.60)  Prec@50=93.75 (96.20)
2023-04-26 09:23:58 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 500/1254]  Batch=0.70 (0.75)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0197 (0.0195)  IoU=85.54 (88.55)  Prec@50=93.75 (96.15)
2023-04-26 09:25:14 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 600/1254]  Batch=0.65 (0.75)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0293 (0.0195)  IoU=86.67 (88.55)  Prec@50=93.75 (96.15)
2023-04-26 09:26:24 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 700/1254]  Batch=0.73 (0.74)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0174 (0.0196)  IoU=83.29 (88.49)  Prec@50=93.75 (96.05)
2023-04-26 09:27:36 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 800/1254]  Batch=0.62 (0.74)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0139 (0.0196)  IoU=87.59 (88.52)  Prec@50=93.75 (96.09)
2023-04-26 09:28:45 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [ 900/1254]  Batch=0.66 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0268 (0.0196)  IoU=93.53 (88.54)  Prec@50=100.00 (96.07)
2023-04-26 09:29:54 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [1000/1254]  Batch=0.75 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0211 (0.0196)  IoU=93.71 (88.51)  Prec@50=100.00 (96.06)
2023-04-26 09:31:08 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [1100/1254]  Batch=0.71 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0263 (0.0196)  IoU=89.03 (88.52)  Prec@50=93.75 (96.07)
2023-04-26 09:32:18 | INFO     | utils.misc:106 - Training: Epoch=[40/50] [1200/1254]  Batch=0.79 (0.73)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0178 (0.0196)  IoU=90.75 (88.53)  Prec@50=100.00 (96.06)
2023-04-26 09:37:44 | INFO     | engine.engine:148 - Evaluation: Epoch=[40/50]  IoU=69.49  Pr@50: 77.61  Pr@60: 71.03  Pr@70: 63.15  Pr@80: 55.36  Pr@90: 38.96  
2023-04-26 09:39:12 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 100/1254]  Batch=0.62 (0.77)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0156 (0.0191)  IoU=83.58 (89.03)  Prec@50=87.50 (96.69)
2023-04-26 09:40:23 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 200/1254]  Batch=0.93 (0.74)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0290 (0.0192)  IoU=80.21 (88.80)  Prec@50=87.50 (96.38)
2023-04-26 09:41:34 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 300/1254]  Batch=0.67 (0.73)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0209 (0.0192)  IoU=90.69 (88.68)  Prec@50=100.00 (96.25)
2023-04-26 09:42:45 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 400/1254]  Batch=0.66 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0176 (0.0191)  IoU=82.19 (88.51)  Prec@50=93.75 (96.00)
2023-04-26 09:43:56 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 500/1254]  Batch=0.65 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0154 (0.0192)  IoU=81.68 (88.52)  Prec@50=87.50 (96.04)
2023-04-26 09:45:04 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 600/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0225 (0.0194)  IoU=94.31 (88.72)  Prec@50=100.00 (96.21)
2023-04-26 09:46:11 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 700/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0206 (0.0194)  IoU=91.30 (88.61)  Prec@50=100.00 (96.07)
2023-04-26 09:47:21 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 800/1254]  Batch=0.63 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0205 (0.0194)  IoU=94.78 (88.62)  Prec@50=100.00 (96.07)
2023-04-26 09:48:29 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [ 900/1254]  Batch=0.65 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0220 (0.0195)  IoU=87.31 (88.56)  Prec@50=93.75 (96.04)
2023-04-26 09:49:37 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [1000/1254]  Batch=0.64 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0237 (0.0195)  IoU=91.36 (88.54)  Prec@50=100.00 (96.02)
2023-04-26 09:50:45 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [1100/1254]  Batch=0.75 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0215 (0.0194)  IoU=90.11 (88.45)  Prec@50=100.00 (95.94)
2023-04-26 09:51:54 | INFO     | utils.misc:106 - Training: Epoch=[41/50] [1200/1254]  Batch=0.74 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0141 (0.0194)  IoU=90.92 (88.49)  Prec@50=100.00 (95.99)
2023-04-26 09:57:18 | INFO     | engine.engine:148 - Evaluation: Epoch=[41/50]  IoU=69.20  Pr@50: 77.30  Pr@60: 70.77  Pr@70: 62.84  Pr@80: 54.60  Pr@90: 38.46  
2023-04-26 09:58:42 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 100/1254]  Batch=0.83 (0.77)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0132 (0.0198)  IoU=91.42 (88.57)  Prec@50=100.00 (96.38)
2023-04-26 09:59:52 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 200/1254]  Batch=0.66 (0.74)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0194 (0.0195)  IoU=87.41 (88.98)  Prec@50=93.75 (96.72)
2023-04-26 10:01:02 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 300/1254]  Batch=0.75 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0134 (0.0196)  IoU=90.09 (88.95)  Prec@50=100.00 (96.62)
2023-04-26 10:02:12 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 400/1254]  Batch=0.78 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0212 (0.0195)  IoU=85.95 (89.00)  Prec@50=93.75 (96.55)
2023-04-26 10:03:20 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 500/1254]  Batch=0.87 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0205 (0.0197)  IoU=90.30 (89.04)  Prec@50=100.00 (96.56)
2023-04-26 10:04:28 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 600/1254]  Batch=0.69 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0230 (0.0196)  IoU=87.34 (88.87)  Prec@50=93.75 (96.40)
2023-04-26 10:05:35 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 700/1254]  Batch=0.65 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0179 (0.0195)  IoU=91.50 (88.66)  Prec@50=100.00 (96.22)
2023-04-26 10:06:44 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 800/1254]  Batch=0.71 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0210 (0.0194)  IoU=83.46 (88.78)  Prec@50=87.50 (96.30)
2023-04-26 10:07:52 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [ 900/1254]  Batch=0.66 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0154 (0.0193)  IoU=86.13 (88.76)  Prec@50=93.75 (96.24)
2023-04-26 10:09:00 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [1000/1254]  Batch=0.61 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0197 (0.0193)  IoU=94.27 (88.68)  Prec@50=100.00 (96.17)
2023-04-26 10:10:07 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [1100/1254]  Batch=0.68 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0178 (0.0193)  IoU=93.38 (88.69)  Prec@50=100.00 (96.18)
2023-04-26 10:11:17 | INFO     | utils.misc:106 - Training: Epoch=[42/50] [1200/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0219 (0.0193)  IoU=84.49 (88.64)  Prec@50=93.75 (96.14)
2023-04-26 10:16:39 | INFO     | engine.engine:148 - Evaluation: Epoch=[42/50]  IoU=68.94  Pr@50: 77.09  Pr@60: 70.50  Pr@70: 62.41  Pr@80: 54.08  Pr@90: 37.40  
2023-04-26 10:18:04 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 100/1254]  Batch=0.71 (0.77)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0168 (0.0187)  IoU=89.34 (88.97)  Prec@50=100.00 (96.06)
2023-04-26 10:19:13 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 200/1254]  Batch=0.69 (0.73)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0277 (0.0183)  IoU=86.39 (88.89)  Prec@50=93.75 (96.22)
2023-04-26 10:20:23 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 300/1254]  Batch=0.65 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0141 (0.0187)  IoU=93.94 (88.80)  Prec@50=100.00 (96.23)
2023-04-26 10:21:33 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 400/1254]  Batch=0.60 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0185 (0.0192)  IoU=78.04 (88.80)  Prec@50=87.50 (96.27)
2023-04-26 10:22:41 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 500/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0210 (0.0191)  IoU=87.70 (88.80)  Prec@50=93.75 (96.24)
2023-04-26 10:23:48 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 600/1254]  Batch=0.64 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0142 (0.0190)  IoU=91.52 (88.76)  Prec@50=100.00 (96.23)
2023-04-26 10:24:56 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 700/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0202 (0.0190)  IoU=91.35 (88.80)  Prec@50=100.00 (96.24)
2023-04-26 10:26:04 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 800/1254]  Batch=0.70 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0138 (0.0189)  IoU=84.06 (88.77)  Prec@50=93.75 (96.23)
2023-04-26 10:27:13 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [ 900/1254]  Batch=0.71 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0160 (0.0190)  IoU=87.68 (88.75)  Prec@50=93.75 (96.19)
2023-04-26 10:28:20 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [1000/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0180 (0.0190)  IoU=85.64 (88.76)  Prec@50=93.75 (96.19)
2023-04-26 10:29:30 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [1100/1254]  Batch=0.64 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0176 (0.0191)  IoU=90.52 (88.69)  Prec@50=93.75 (96.12)
2023-04-26 10:30:38 | INFO     | utils.misc:106 - Training: Epoch=[43/50] [1200/1254]  Batch=0.73 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0130 (0.0191)  IoU=84.85 (88.61)  Prec@50=93.75 (96.03)
2023-04-26 10:36:04 | INFO     | engine.engine:148 - Evaluation: Epoch=[43/50]  IoU=68.69  Pr@50: 77.02  Pr@60: 70.52  Pr@70: 62.53  Pr@80: 53.87  Pr@90: 37.54  
2023-04-26 10:37:28 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 100/1254]  Batch=0.69 (0.77)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0205 (0.0210)  IoU=86.34 (88.83)  Prec@50=93.75 (96.50)
2023-04-26 10:38:37 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 200/1254]  Batch=0.63 (0.73)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0200 (0.0197)  IoU=92.99 (88.92)  Prec@50=100.00 (96.31)
2023-04-26 10:39:46 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 300/1254]  Batch=0.61 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0213 (0.0196)  IoU=92.00 (88.90)  Prec@50=100.00 (96.31)
2023-04-26 10:40:53 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 400/1254]  Batch=0.69 (0.71)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0204 (0.0194)  IoU=76.01 (88.81)  Prec@50=81.25 (96.27)
2023-04-26 10:42:03 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 500/1254]  Batch=0.67 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0139 (0.0191)  IoU=79.94 (88.79)  Prec@50=87.50 (96.29)
2023-04-26 10:43:12 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 600/1254]  Batch=0.63 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0161 (0.0191)  IoU=90.52 (88.78)  Prec@50=93.75 (96.27)
2023-04-26 10:44:19 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 700/1254]  Batch=0.69 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0215 (0.0192)  IoU=86.85 (88.75)  Prec@50=87.50 (96.28)
2023-04-26 10:45:28 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 800/1254]  Batch=0.61 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0172 (0.0191)  IoU=86.77 (88.73)  Prec@50=93.75 (96.20)
2023-04-26 10:46:36 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [ 900/1254]  Batch=0.70 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0136 (0.0191)  IoU=88.84 (88.79)  Prec@50=100.00 (96.26)
2023-04-26 10:47:46 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [1000/1254]  Batch=0.67 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0230 (0.0191)  IoU=87.59 (88.73)  Prec@50=93.75 (96.19)
2023-04-26 10:48:55 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [1100/1254]  Batch=0.69 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0249 (0.0192)  IoU=92.76 (88.73)  Prec@50=100.00 (96.16)
2023-04-26 10:50:04 | INFO     | utils.misc:106 - Training: Epoch=[44/50] [1200/1254]  Batch=0.69 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0124 (0.0191)  IoU=90.37 (88.66)  Prec@50=93.75 (96.08)
2023-04-26 10:55:28 | INFO     | engine.engine:148 - Evaluation: Epoch=[44/50]  IoU=68.88  Pr@50: 77.14  Pr@60: 70.52  Pr@70: 62.27  Pr@80: 54.16  Pr@90: 37.79  
2023-04-26 10:56:50 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 100/1254]  Batch=0.68 (0.75)  Data=0.00 (0.08)  Lr=0.000010  Loss=0.0159 (0.0186)  IoU=90.18 (88.21)  Prec@50=93.75 (95.62)
2023-04-26 10:57:59 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 200/1254]  Batch=0.63 (0.72)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0162 (0.0191)  IoU=86.57 (88.63)  Prec@50=93.75 (96.03)
2023-04-26 10:59:08 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 300/1254]  Batch=0.67 (0.71)  Data=0.00 (0.03)  Lr=0.000010  Loss=0.0137 (0.0190)  IoU=90.73 (88.64)  Prec@50=100.00 (96.08)
2023-04-26 11:00:16 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 400/1254]  Batch=0.61 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0202 (0.0189)  IoU=81.57 (88.78)  Prec@50=87.50 (96.14)
2023-04-26 11:01:24 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 500/1254]  Batch=0.69 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0215 (0.0189)  IoU=91.73 (88.88)  Prec@50=93.75 (96.28)
2023-04-26 11:02:31 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 600/1254]  Batch=0.62 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0161 (0.0190)  IoU=93.95 (88.82)  Prec@50=100.00 (96.24)
2023-04-26 11:03:38 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 700/1254]  Batch=0.63 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0143 (0.0189)  IoU=92.71 (88.68)  Prec@50=100.00 (96.18)
2023-04-26 11:04:47 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 800/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0187 (0.0189)  IoU=94.74 (88.63)  Prec@50=100.00 (96.11)
2023-04-26 11:05:55 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [ 900/1254]  Batch=0.63 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0139 (0.0189)  IoU=87.76 (88.69)  Prec@50=93.75 (96.11)
2023-04-26 11:07:03 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [1000/1254]  Batch=0.76 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0152 (0.0189)  IoU=76.76 (88.64)  Prec@50=81.25 (96.03)
2023-04-26 11:08:12 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [1100/1254]  Batch=0.63 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0171 (0.0188)  IoU=92.37 (88.66)  Prec@50=100.00 (96.01)
2023-04-26 11:09:20 | INFO     | utils.misc:106 - Training: Epoch=[45/50] [1200/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0215 (0.0189)  IoU=94.29 (88.67)  Prec@50=100.00 (96.03)
2023-04-26 11:14:50 | INFO     | engine.engine:148 - Evaluation: Epoch=[45/50]  IoU=68.53  Pr@50: 76.72  Pr@60: 69.97  Pr@70: 61.99  Pr@80: 53.97  Pr@90: 37.95  
2023-04-26 11:16:12 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 100/1254]  Batch=0.69 (0.76)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0116 (0.0188)  IoU=87.53 (89.12)  Prec@50=100.00 (96.56)
2023-04-26 11:17:20 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 200/1254]  Batch=0.66 (0.72)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0163 (0.0189)  IoU=89.26 (88.91)  Prec@50=100.00 (96.34)
2023-04-26 11:18:27 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 300/1254]  Batch=0.65 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0218 (0.0189)  IoU=95.43 (88.91)  Prec@50=100.00 (96.35)
2023-04-26 11:19:35 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 400/1254]  Batch=0.60 (0.69)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0245 (0.0189)  IoU=90.42 (88.99)  Prec@50=100.00 (96.39)
2023-04-26 11:20:42 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 500/1254]  Batch=0.69 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0183 (0.0187)  IoU=95.26 (89.08)  Prec@50=100.00 (96.41)
2023-04-26 11:21:50 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 600/1254]  Batch=0.74 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0128 (0.0187)  IoU=82.56 (88.95)  Prec@50=87.50 (96.30)
2023-04-26 11:22:59 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 700/1254]  Batch=0.78 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0183 (0.0187)  IoU=91.27 (88.88)  Prec@50=100.00 (96.24)
2023-04-26 11:24:08 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 800/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0268 (0.0188)  IoU=94.31 (88.88)  Prec@50=100.00 (96.30)
2023-04-26 11:25:17 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [ 900/1254]  Batch=0.70 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0120 (0.0188)  IoU=85.27 (88.93)  Prec@50=93.75 (96.35)
2023-04-26 11:26:25 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [1000/1254]  Batch=0.64 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0191 (0.0188)  IoU=86.22 (88.89)  Prec@50=93.75 (96.33)
2023-04-26 11:27:34 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [1100/1254]  Batch=0.68 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0174 (0.0188)  IoU=88.90 (88.91)  Prec@50=93.75 (96.34)
2023-04-26 11:28:45 | INFO     | utils.misc:106 - Training: Epoch=[46/50] [1200/1254]  Batch=0.74 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0170 (0.0188)  IoU=90.58 (88.83)  Prec@50=100.00 (96.26)
2023-04-26 11:34:08 | INFO     | engine.engine:148 - Evaluation: Epoch=[46/50]  IoU=69.37  Pr@50: 77.59  Pr@60: 70.96  Pr@70: 62.89  Pr@80: 54.70  Pr@90: 38.22  
2023-04-26 11:35:31 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 100/1254]  Batch=0.71 (0.77)  Data=0.00 (0.08)  Lr=0.000010  Loss=0.0145 (0.0188)  IoU=80.84 (88.43)  Prec@50=87.50 (95.88)
2023-04-26 11:36:40 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 200/1254]  Batch=0.64 (0.73)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0186 (0.0189)  IoU=86.01 (88.71)  Prec@50=93.75 (96.19)
2023-04-26 11:37:48 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 300/1254]  Batch=0.65 (0.71)  Data=0.00 (0.03)  Lr=0.000010  Loss=0.0181 (0.0189)  IoU=94.70 (88.91)  Prec@50=100.00 (96.38)
2023-04-26 11:38:56 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 400/1254]  Batch=0.64 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0327 (0.0188)  IoU=87.29 (88.88)  Prec@50=93.75 (96.28)
2023-04-26 11:40:04 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 500/1254]  Batch=0.64 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0230 (0.0187)  IoU=90.31 (88.96)  Prec@50=100.00 (96.38)
2023-04-26 11:41:13 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 600/1254]  Batch=0.73 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0393 (0.0188)  IoU=92.49 (88.91)  Prec@50=100.00 (96.23)
2023-04-26 11:42:21 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 700/1254]  Batch=0.69 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0170 (0.0187)  IoU=94.08 (88.85)  Prec@50=100.00 (96.18)
2023-04-26 11:43:28 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 800/1254]  Batch=0.63 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0183 (0.0187)  IoU=85.45 (88.82)  Prec@50=93.75 (96.16)
2023-04-26 11:44:37 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [ 900/1254]  Batch=0.64 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0190 (0.0186)  IoU=91.07 (88.81)  Prec@50=100.00 (96.17)
2023-04-26 11:45:47 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [1000/1254]  Batch=0.71 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0169 (0.0187)  IoU=85.30 (88.73)  Prec@50=87.50 (96.06)
2023-04-26 11:46:57 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [1100/1254]  Batch=0.72 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0209 (0.0187)  IoU=94.68 (88.71)  Prec@50=100.00 (96.02)
2023-04-26 11:48:06 | INFO     | utils.misc:106 - Training: Epoch=[47/50] [1200/1254]  Batch=0.64 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0204 (0.0187)  IoU=90.98 (88.73)  Prec@50=93.75 (96.07)
2023-04-26 11:53:30 | INFO     | engine.engine:148 - Evaluation: Epoch=[47/50]  IoU=69.64  Pr@50: 77.99  Pr@60: 71.31  Pr@70: 63.18  Pr@80: 54.90  Pr@90: 38.45  
2023-04-26 11:54:58 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 100/1254]  Batch=0.69 (0.76)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0181 (0.0200)  IoU=76.92 (89.10)  Prec@50=87.50 (96.81)
2023-04-26 11:56:06 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 200/1254]  Batch=0.64 (0.72)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0217 (0.0190)  IoU=86.39 (89.06)  Prec@50=93.75 (96.62)
2023-04-26 11:57:14 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 300/1254]  Batch=0.64 (0.71)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0137 (0.0189)  IoU=85.80 (89.00)  Prec@50=93.75 (96.56)
2023-04-26 11:58:23 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 400/1254]  Batch=0.81 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0176 (0.0188)  IoU=83.38 (88.92)  Prec@50=87.50 (96.27)
2023-04-26 11:59:31 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 500/1254]  Batch=0.68 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0255 (0.0187)  IoU=91.63 (88.99)  Prec@50=100.00 (96.39)
2023-04-26 12:00:38 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 600/1254]  Batch=0.67 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0198 (0.0188)  IoU=91.36 (88.97)  Prec@50=100.00 (96.42)
2023-04-26 12:01:47 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 700/1254]  Batch=0.67 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0204 (0.0188)  IoU=87.41 (88.91)  Prec@50=93.75 (96.33)
2023-04-26 12:02:56 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 800/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0189 (0.0189)  IoU=94.73 (88.93)  Prec@50=100.00 (96.28)
2023-04-26 12:04:07 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [ 900/1254]  Batch=0.73 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0175 (0.0189)  IoU=91.12 (88.90)  Prec@50=100.00 (96.25)
2023-04-26 12:05:17 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [1000/1254]  Batch=0.78 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0156 (0.0189)  IoU=93.38 (88.88)  Prec@50=100.00 (96.22)
2023-04-26 12:06:26 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [1100/1254]  Batch=0.64 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0129 (0.0189)  IoU=86.10 (88.80)  Prec@50=87.50 (96.12)
2023-04-26 12:07:34 | INFO     | utils.misc:106 - Training: Epoch=[48/50] [1200/1254]  Batch=0.65 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0142 (0.0189)  IoU=87.65 (88.82)  Prec@50=93.75 (96.15)
2023-04-26 12:12:57 | INFO     | engine.engine:148 - Evaluation: Epoch=[48/50]  IoU=69.18  Pr@50: 77.50  Pr@60: 70.85  Pr@70: 62.43  Pr@80: 54.45  Pr@90: 38.33  
2023-04-26 12:14:18 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 100/1254]  Batch=0.63 (0.75)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0212 (0.0186)  IoU=93.73 (88.78)  Prec@50=100.00 (95.94)
2023-04-26 12:15:27 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 200/1254]  Batch=0.67 (0.72)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0192 (0.0182)  IoU=90.93 (88.96)  Prec@50=100.00 (96.25)
2023-04-26 12:16:36 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 300/1254]  Batch=0.66 (0.71)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0178 (0.0185)  IoU=86.64 (88.93)  Prec@50=93.75 (96.21)
2023-04-26 12:17:44 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 400/1254]  Batch=0.64 (0.70)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0198 (0.0185)  IoU=88.49 (89.06)  Prec@50=93.75 (96.34)
2023-04-26 12:18:52 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 500/1254]  Batch=0.72 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0185 (0.0186)  IoU=92.29 (89.06)  Prec@50=100.00 (96.35)
2023-04-26 12:20:00 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 600/1254]  Batch=0.77 (0.69)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0171 (0.0186)  IoU=87.00 (89.12)  Prec@50=93.75 (96.48)
2023-04-26 12:21:12 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 700/1254]  Batch=0.81 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0183 (0.0186)  IoU=90.47 (89.17)  Prec@50=100.00 (96.54)
2023-04-26 12:22:25 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 800/1254]  Batch=0.74 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0190 (0.0186)  IoU=93.22 (89.07)  Prec@50=100.00 (96.41)
2023-04-26 12:23:37 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [ 900/1254]  Batch=0.61 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0208 (0.0186)  IoU=90.17 (89.14)  Prec@50=100.00 (96.47)
2023-04-26 12:24:48 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [1000/1254]  Batch=0.70 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0180 (0.0187)  IoU=82.01 (89.10)  Prec@50=87.50 (96.44)
2023-04-26 12:25:58 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [1100/1254]  Batch=0.80 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0203 (0.0187)  IoU=92.27 (89.10)  Prec@50=100.00 (96.43)
2023-04-26 12:27:07 | INFO     | utils.misc:106 - Training: Epoch=[49/50] [1200/1254]  Batch=0.66 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0174 (0.0187)  IoU=93.02 (89.11)  Prec@50=100.00 (96.42)
2023-04-26 12:32:37 | INFO     | engine.engine:148 - Evaluation: Epoch=[49/50]  IoU=67.93  Pr@50: 76.36  Pr@60: 69.42  Pr@70: 61.13  Pr@80: 52.37  Pr@90: 36.59  
2023-04-26 12:34:02 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 100/1254]  Batch=0.63 (0.78)  Data=0.00 (0.07)  Lr=0.000010  Loss=0.0160 (0.0179)  IoU=83.47 (88.88)  Prec@50=87.50 (96.12)
2023-04-26 12:35:11 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 200/1254]  Batch=0.66 (0.74)  Data=0.00 (0.04)  Lr=0.000010  Loss=0.0149 (0.0180)  IoU=93.30 (89.35)  Prec@50=100.00 (96.56)
2023-04-26 12:36:21 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 300/1254]  Batch=0.69 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0199 (0.0181)  IoU=91.58 (89.28)  Prec@50=100.00 (96.56)
2023-04-26 12:37:30 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 400/1254]  Batch=0.66 (0.72)  Data=0.00 (0.02)  Lr=0.000010  Loss=0.0199 (0.0182)  IoU=82.97 (89.31)  Prec@50=87.50 (96.66)
2023-04-26 12:38:42 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 500/1254]  Batch=0.70 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0183 (0.0182)  IoU=85.61 (89.21)  Prec@50=93.75 (96.53)
2023-04-26 12:39:54 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 600/1254]  Batch=0.79 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0215 (0.0182)  IoU=88.46 (89.15)  Prec@50=100.00 (96.53)
2023-04-26 12:41:05 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 700/1254]  Batch=0.63 (0.72)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0164 (0.0183)  IoU=85.29 (89.06)  Prec@50=93.75 (96.41)
2023-04-26 12:42:14 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 800/1254]  Batch=0.68 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0274 (0.0184)  IoU=90.19 (89.13)  Prec@50=100.00 (96.48)
2023-04-26 12:43:24 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [ 900/1254]  Batch=0.73 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0177 (0.0185)  IoU=91.10 (89.12)  Prec@50=93.75 (96.47)
2023-04-26 12:44:33 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [1000/1254]  Batch=0.67 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0200 (0.0186)  IoU=92.43 (89.13)  Prec@50=100.00 (96.48)
2023-04-26 12:45:42 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [1100/1254]  Batch=0.70 (0.71)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0203 (0.0185)  IoU=90.02 (89.07)  Prec@50=100.00 (96.43)
2023-04-26 12:46:50 | INFO     | utils.misc:106 - Training: Epoch=[50/50] [1200/1254]  Batch=0.62 (0.70)  Data=0.00 (0.01)  Lr=0.000010  Loss=0.0140 (0.0186)  IoU=80.96 (89.05)  Prec@50=93.75 (96.41)
2023-04-26 12:52:17 | INFO     | engine.engine:148 - Evaluation: Epoch=[50/50]  IoU=69.33  Pr@50: 77.64  Pr@60: 71.15  Pr@70: 63.25  Pr@80: 54.84  Pr@90: 37.89  
2023-04-26 12:52:24 | INFO     | __mp_main__:230 - * Best IoU=0.6964228234457639 * 
2023-04-26 12:52:24 | INFO     | __mp_main__:233 - * Training time 18:21:48 *
